{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6399b35-6488-4a16-92d6-d7d9acfe944d",
   "metadata": {
    "id": "e6399b35-6488-4a16-92d6-d7d9acfe944d",
    "outputId": "689dccdb-fb13-4a4c-8968-ae9f4c6fbf56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH_CUDA_ALLOC_CONF set to: expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "print(f\"PYTORCH_CUDA_ALLOC_CONF set to: {os.environ.get('PYTORCH_CUDA_ALLOC_CONF')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a95315-21c1-41de-a409-09f3319d9b68",
   "metadata": {
    "id": "98a95315-21c1-41de-a409-09f3319d9b68",
    "outputId": "3902861a-46cd-4c01-af9c-d236f1cd9edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netCDF4 in /opt/conda/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.10.3)\n",
      "Requirement already satisfied: cftime in /opt/conda/lib/python3.11/site-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from netCDF4) (2024.8.30)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from netCDF4) (2.1.2)\n",
      "Requirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (24.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (2.5.1+cu121)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (0.14.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (72.1.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install netCDF4 torchmetrics matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "BrhYI9SSVo_t",
   "metadata": {
    "id": "BrhYI9SSVo_t",
    "outputId": "0a7831e7-cd07-446e-badb-1dc03a590380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f803dd-b188-46c0-96a9-496c8ae389ea",
   "metadata": {
    "id": "02f803dd-b188-46c0-96a9-496c8ae389ea",
    "outputId": "f621b64f-534d-4e6c-c68b-e27c2c70e147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1mTl6jomSku9xiDvtw8o_qia4iRFUGjYY\n",
      "From (redirected): https://drive.google.com/uc?id=1mTl6jomSku9xiDvtw8o_qia4iRFUGjYY&confirm=t&uuid=4109d594-fa90-44a2-9d73-1e9d128bfce1\n",
      "To: /home/sample.tar.gz\n",
      "100%|██████████████████████████████████████| 3.00G/3.00G [00:37<00:00, 79.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "!cd /home\n",
    "!gdown \"https://drive.google.com/uc?id=1mTl6jomSku9xiDvtw8o_qia4iRFUGjYY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46053838-a6f8-4835-9572-b7a409d2f94a",
   "metadata": {
    "id": "46053838-a6f8-4835-9572-b7a409d2f94a",
    "outputId": "6a68f00c-b475-407f-90a5-ebbb4324abe1"
   },
   "outputs": [],
   "source": [
    "!tar -xzvf /home/sample.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec770cc-2e7d-4ca2-b908-7ee098a5b596",
   "metadata": {
    "id": "9ec770cc-2e7d-4ca2-b908-7ee098a5b596",
    "outputId": "392d73ea-0d1f-4f22-b60b-3d378251092b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version PyTorch built with: 12.1\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA H100 NVL\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version PyTorch built with: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb893018-e461-4170-bd08-9cefba404415",
   "metadata": {
    "id": "eb893018-e461-4170-bd08-9cefba404415",
    "outputId": "659baab6-c872-42d7-ee99-a44ea00fd074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio '/home/sample' contiene 370 subcarpetas (directorios).\n",
      "\n",
      "Algunas de las subcarpetas encontradas:\n",
      "- 2015122110\n",
      "- 201511021\n",
      "- 201612177\n",
      "- 201702244\n",
      "- 201612171\n",
      "- 201603033\n",
      "- 201703276\n",
      "- 201812263\n",
      "- 2016020417\n",
      "- 201601232\n",
      "... y 360 más.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# El path base que quieres inspeccionar\n",
    "base_path = \"/home/sample\"\n",
    "\n",
    "# Verificar si el path base existe\n",
    "if not os.path.exists(base_path):\n",
    "    print(f\"Error: El directorio base '{base_path}' no existe.\")\n",
    "else:\n",
    "    # Listar todos los contenidos del directorio base\n",
    "    try:\n",
    "        all_contents = os.listdir(base_path)\n",
    "\n",
    "        # Filtrar para quedarnos solo con los directorios\n",
    "        subdirectories = [d for d in all_contents if os.path.isdir(os.path.join(base_path, d))]\n",
    "\n",
    "        # Contar la cantidad de subdirectorios\n",
    "        num_subdirectories = len(subdirectories)\n",
    "\n",
    "        print(f\"El directorio '{base_path}' contiene {num_subdirectories} subcarpetas (directorios).\")\n",
    "\n",
    "        # Opcional: Imprimir los primeros N nombres de subcarpetas para verificar\n",
    "        if num_subdirectories > 0:\n",
    "            print(\"\\nAlgunas de las subcarpetas encontradas:\")\n",
    "            for i, subdir_name in enumerate(subdirectories):\n",
    "                if i < 10: # Imprime las primeras 10 (o menos si hay menos)\n",
    "                    print(f\"- {subdir_name}\")\n",
    "                else:\n",
    "                    break\n",
    "            if num_subdirectories > 10:\n",
    "                print(f\"... y {num_subdirectories - 10} más.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al intentar listar los contenidos de '{base_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8c5fd3-2adb-477c-a88c-43534f9e647a",
   "metadata": {
    "id": "da8c5fd3-2adb-477c-a88c-43534f9e647a",
    "outputId": "13641949-af46-44a2-a46a-c47df8cc0195"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 21:25:23,094 - INFO - Semillas configuradas con valor: 42\n",
      "2025-06-01 21:25:23,095 - INFO - Usando muestra aleatoria de 10 secuencias.\n",
      "2025-06-01 21:25:23,096 - INFO - Total secuencias a usar: 10.\n",
      "2025-06-01 21:25:23,096 - INFO - Entrenamiento: 8 sec. Validación: 2 sec.\n",
      "2025-06-01 21:25:23,134 - INFO - Archivo /home/sample/202002023/202200.nc: Min=-30.00, Max=63.99\n",
      "2025-06-01 21:25:23,170 - INFO - Archivo /home/sample/201911116/200242.nc: Min=-30.00, Max=60.41\n",
      "2025-06-01 21:25:23,205 - INFO - Archivo /home/sample/2020020314/210448.nc: Min=-30.00, Max=62.44\n",
      "2025-06-01 21:25:23,238 - INFO - Archivo /home/sample/201812274/014714.nc: Min=-30.00, Max=62.59\n",
      "2025-06-01 21:25:23,273 - INFO - Archivo /home/sample/2017032915/140403.nc: Min=-30.00, Max=63.42\n",
      "2025-06-01 21:25:23,306 - INFO - Archivo /home/sample/2020020315/214849.nc: Min=-30.00, Max=62.52\n",
      "2025-06-01 21:25:23,341 - INFO - Archivo /home/sample/201911247/162417.nc: Min=-30.00, Max=60.54\n",
      "2025-06-01 21:25:23,375 - INFO - Archivo /home/sample/2017032713/103508.nc: Min=-30.00, Max=65.46\n",
      "2025-06-01 21:25:23,378 - INFO - RadarDataset inicializado con 8 secuencias válidas.\n",
      "2025-06-01 21:25:23,416 - INFO - Archivo /home/sample/201511169/210649.nc: Min=-30.00, Max=58.14\n",
      "2025-06-01 21:25:23,452 - INFO - Archivo /home/sample/201911106/214525.nc: Min=-30.00, Max=61.14\n",
      "2025-06-01 21:25:23,455 - INFO - RadarDataset inicializado con 2 secuencias válidas.\n",
      "2025-06-01 21:25:23,462 - INFO - Modelo ConvLSTM3D_Enhanced creado: 2 capas, Hidden dims: [32, 32], LayerNorm: True, PredSteps: 1\n",
      "2025-06-01 21:25:23,462 - INFO - Arquitectura del modelo:\n",
      "ConvLSTM3D_Enhanced(\n",
      "  (layers): ModuleList(\n",
      "    (0): ConvLSTM2DLayer(\n",
      "      (cell): ConvLSTMCell(\n",
      "        (conv): Conv2d(33, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): ConvLSTM2DLayer(\n",
      "      (cell): ConvLSTMCell(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer_norms): ModuleList(\n",
      "    (0-1): 2 x LayerNorm((32, 500, 500), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (output_conv): Conv3d(32, 1, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "2025-06-01 21:25:23,462 - INFO - Número total de parámetros entrenables: 32,112,289\n",
      "2025-06-01 21:25:23,463 - INFO - No se encontró modelo pre-entrenado. Entrenando desde cero...\n",
      "2025-06-01 21:25:23,463 - INFO - Usando dispositivo: cuda\n",
      "2025-06-01 21:25:23,476 - INFO - Iniciando entrenamiento: 2 épocas, LR: 0.001, Batch (efectivo): 1\n",
      "2025-06-01 21:25:23,537 - INFO - Inicio Época 1 - Memoria GPU Asignada: 0.24 GB, Reservada: 0.26 GB\n",
      "2025-06-01 21:25:23,648 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:23,649 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0036\n",
      "2025-06-01 21:25:23,730 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0072\n",
      "2025-06-01 21:25:23,735 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0035\n",
      "2025-06-01 21:25:23,811 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0068\n",
      "2025-06-01 21:25:23,818 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0032\n",
      "2025-06-01 21:25:23,897 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0075\n",
      "2025-06-01 21:25:23,902 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0031\n",
      "2025-06-01 21:25:23,978 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0072\n",
      "2025-06-01 21:25:23,983 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0027\n",
      "2025-06-01 21:25:24,058 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0080\n",
      "2025-06-01 21:25:24,064 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0027\n",
      "2025-06-01 21:25:24,139 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:24,149 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0026\n",
      "2025-06-01 21:25:24,188 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:24,199 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0026\n",
      "2025-06-01 21:25:24,468 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0052\n",
      "2025-06-01 21:25:24,488 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0077\n",
      "2025-06-01 21:25:24,550 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:24,569 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:24,626 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0055\n",
      "2025-06-01 21:25:24,646 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:24,703 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0057\n",
      "2025-06-01 21:25:24,723 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:24,780 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0055\n",
      "2025-06-01 21:25:24,799 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0087\n",
      "2025-06-01 21:25:24,856 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0059\n",
      "2025-06-01 21:25:24,875 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0086\n",
      "2025-06-01 21:25:24,932 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0061\n",
      "2025-06-01 21:25:24,964 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:24,979 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0061\n",
      "2025-06-01 21:25:25,010 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:25,161 - INFO -   Predicciones (normalizadas): Min=0.0000, Max=1.0000, Mean=0.5034\n",
      "2025-06-01 21:25:25,162 - INFO -   Objetivos y (normalizados): Min=0.0000, Max=1.0000, Mean=0.0026\n",
      "2025-06-01 21:25:25,294 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0019\n",
      "2025-06-01 21:25:25,368 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0020\n",
      "2025-06-01 21:25:25,440 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:25,509 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0018\n",
      "2025-06-01 21:25:25,580 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:25,650 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:25,720 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:25,752 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:26,003 - INFO - Época 1/2 [1/8] - Pérdida (batch): 0.253819\n",
      "2025-06-01 21:25:26,081 - INFO - dbz_final: Min=0.0000, Max=0.9914, Mean=0.0038\n",
      "2025-06-01 21:25:26,157 - INFO - dbz_final: Min=0.0000, Max=0.9899, Mean=0.0040\n",
      "2025-06-01 21:25:26,227 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0041\n",
      "2025-06-01 21:25:26,295 - INFO - dbz_final: Min=0.0000, Max=0.9915, Mean=0.0042\n",
      "2025-06-01 21:25:26,363 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0040\n",
      "2025-06-01 21:25:26,429 - INFO - dbz_final: Min=0.0000, Max=0.9949, Mean=0.0044\n",
      "2025-06-01 21:25:26,496 - INFO - dbz_final: Min=0.0000, Max=0.9990, Mean=0.0047\n",
      "2025-06-01 21:25:26,527 - INFO - Output tensor: Min=0.0000, Max=0.9990, Mean=0.0047\n",
      "2025-06-01 21:25:27,571 - INFO - Época 1/2 [2/8] - Pérdida (batch): 0.255369\n",
      "2025-06-01 21:25:27,651 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0150\n",
      "2025-06-01 21:25:27,722 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0144\n",
      "2025-06-01 21:25:27,790 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0143\n",
      "2025-06-01 21:25:27,858 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0133\n",
      "2025-06-01 21:25:27,926 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0124\n",
      "2025-06-01 21:25:27,994 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0136\n",
      "2025-06-01 21:25:28,060 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0126\n",
      "2025-06-01 21:25:28,091 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0126\n",
      "2025-06-01 21:25:29,139 - INFO - Época 1/2 [3/8] - Pérdida (batch): 0.255184\n",
      "2025-06-01 21:25:29,218 - INFO - dbz_final: Min=0.0000, Max=0.9945, Mean=0.0034\n",
      "2025-06-01 21:25:29,290 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0039\n",
      "2025-06-01 21:25:29,358 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:29,426 - INFO - dbz_final: Min=0.0000, Max=0.9748, Mean=0.0034\n",
      "2025-06-01 21:25:29,492 - INFO - dbz_final: Min=0.0000, Max=0.9905, Mean=0.0031\n",
      "2025-06-01 21:25:29,560 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0033\n",
      "2025-06-01 21:25:29,628 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:29,659 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:30,707 - INFO - Época 1/2 [4/8] - Pérdida (batch): 0.254672\n",
      "2025-06-01 21:25:32,273 - INFO - Época 1/2 [5/8] - Pérdida (batch): 0.252034\n",
      "2025-06-01 21:25:33,839 - INFO - Época 1/2 [6/8] - Pérdida (batch): 0.253066\n",
      "2025-06-01 21:25:35,407 - INFO - Época 1/2 [7/8] - Pérdida (batch): 0.257150\n",
      "2025-06-01 21:25:36,975 - INFO - Época 1/2 [8/8] - Pérdida (batch): 0.005556\n",
      "2025-06-01 21:25:37,118 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:37,118 - INFO - dbz_final: Min=0.0000, Max=0.9991, Mean=0.0055\n",
      "2025-06-01 21:25:37,203 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:37,203 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0054\n",
      "2025-06-01 21:25:37,286 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:37,289 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0086\n",
      "2025-06-01 21:25:37,369 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0052\n",
      "2025-06-01 21:25:37,370 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:37,452 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:37,454 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0094\n",
      "2025-06-01 21:25:37,538 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0091\n",
      "2025-06-01 21:25:37,538 - INFO - dbz_final: Min=0.0000, Max=0.9932, Mean=0.0053\n",
      "2025-06-01 21:25:37,623 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "2025-06-01 21:25:37,623 - INFO - dbz_final: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:37,675 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "2025-06-01 21:25:37,675 - INFO - Output tensor: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:39,386 - INFO - Época 1 completada en 15.85s. Pérdida (train): 0.223356, Pérdida (val): 0.004874\n",
      "2025-06-01 21:25:39,544 - INFO - Mejor modelo guardado (Pérdida Val: 0.004874)\n",
      "2025-06-01 21:25:39,692 - INFO - Checkpoint guardado en la época 1\n",
      "2025-06-01 21:25:39,753 - INFO - Inicio Época 2 - Memoria GPU Asignada: 0.76 GB, Reservada: 0.84 GB\n",
      "2025-06-01 21:25:39,877 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0019\n",
      "2025-06-01 21:25:39,879 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0150\n",
      "2025-06-01 21:25:39,964 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0020\n",
      "2025-06-01 21:25:39,967 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0144\n",
      "2025-06-01 21:25:40,046 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:40,052 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0143\n",
      "2025-06-01 21:25:40,127 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0018\n",
      "2025-06-01 21:25:40,134 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0133\n",
      "2025-06-01 21:25:40,210 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:40,214 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0124\n",
      "2025-06-01 21:25:40,292 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:40,297 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0136\n",
      "2025-06-01 21:25:40,371 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:40,380 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0126\n",
      "2025-06-01 21:25:40,420 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:40,428 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0126\n",
      "2025-06-01 21:25:40,688 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:40,713 - INFO - dbz_final: Min=0.0000, Max=0.9914, Mean=0.0038\n",
      "2025-06-01 21:25:40,762 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0072\n",
      "2025-06-01 21:25:40,788 - INFO - dbz_final: Min=0.0000, Max=0.9899, Mean=0.0040\n",
      "2025-06-01 21:25:40,837 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0068\n",
      "2025-06-01 21:25:40,866 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0041\n",
      "2025-06-01 21:25:40,913 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0075\n",
      "2025-06-01 21:25:40,942 - INFO - dbz_final: Min=0.0000, Max=0.9915, Mean=0.0042\n",
      "2025-06-01 21:25:40,987 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0072\n",
      "2025-06-01 21:25:41,017 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0040\n",
      "2025-06-01 21:25:41,060 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0080\n",
      "2025-06-01 21:25:41,089 - INFO - dbz_final: Min=0.0000, Max=0.9949, Mean=0.0044\n",
      "2025-06-01 21:25:41,134 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:41,179 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:41,179 - INFO - dbz_final: Min=0.0000, Max=0.9990, Mean=0.0047\n",
      "2025-06-01 21:25:41,229 - INFO - Output tensor: Min=0.0000, Max=0.9990, Mean=0.0047\n",
      "2025-06-01 21:25:41,482 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0052\n",
      "2025-06-01 21:25:41,554 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:41,624 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0055\n",
      "2025-06-01 21:25:41,694 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0057\n",
      "2025-06-01 21:25:41,764 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0055\n",
      "2025-06-01 21:25:41,833 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0059\n",
      "2025-06-01 21:25:41,904 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0061\n",
      "2025-06-01 21:25:41,935 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0061\n",
      "2025-06-01 21:25:42,235 - INFO - Época 2/2 [1/8] - Pérdida (batch): 0.008567\n",
      "2025-06-01 21:25:42,308 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0036\n",
      "2025-06-01 21:25:42,381 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0035\n",
      "2025-06-01 21:25:42,451 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0032\n",
      "2025-06-01 21:25:42,522 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0031\n",
      "2025-06-01 21:25:42,592 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0027\n",
      "2025-06-01 21:25:42,662 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0027\n",
      "2025-06-01 21:25:42,732 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0026\n",
      "2025-06-01 21:25:42,763 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0026\n",
      "2025-06-01 21:25:43,802 - INFO - Época 2/2 [2/8] - Pérdida (batch): 0.000763\n",
      "2025-06-01 21:25:43,874 - INFO - dbz_final: Min=0.0000, Max=0.9945, Mean=0.0034\n",
      "2025-06-01 21:25:43,947 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0039\n",
      "2025-06-01 21:25:44,017 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:44,088 - INFO - dbz_final: Min=0.0000, Max=0.9748, Mean=0.0034\n",
      "2025-06-01 21:25:44,157 - INFO - dbz_final: Min=0.0000, Max=0.9905, Mean=0.0031\n",
      "2025-06-01 21:25:44,226 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0033\n",
      "2025-06-01 21:25:44,296 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:44,327 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:45,370 - INFO - Época 2/2 [3/8] - Pérdida (batch): 0.000991\n",
      "2025-06-01 21:25:45,441 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0077\n",
      "2025-06-01 21:25:45,513 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:45,585 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:45,655 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:45,726 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0087\n",
      "2025-06-01 21:25:45,795 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0086\n",
      "2025-06-01 21:25:45,864 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:45,895 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:46,940 - INFO - Época 2/2 [4/8] - Pérdida (batch): 0.002652\n",
      "2025-06-01 21:25:48,509 - INFO - Época 2/2 [5/8] - Pérdida (batch): 0.001149\n",
      "2025-06-01 21:25:50,078 - INFO - Época 2/2 [6/8] - Pérdida (batch): 0.000655\n",
      "2025-06-01 21:25:51,648 - INFO - Época 2/2 [7/8] - Pérdida (batch): 0.000887\n",
      "2025-06-01 21:25:53,216 - INFO - Época 2/2 [8/8] - Pérdida (batch): 0.001900\n",
      "2025-06-01 21:25:53,377 - INFO - dbz_final: Min=0.0000, Max=0.9991, Mean=0.0055\n",
      "2025-06-01 21:25:53,384 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:53,460 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0054\n",
      "2025-06-01 21:25:53,468 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:53,537 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:53,551 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0086\n",
      "2025-06-01 21:25:53,611 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0052\n",
      "2025-06-01 21:25:53,629 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:53,683 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:53,706 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0094\n",
      "2025-06-01 21:25:53,759 - INFO - dbz_final: Min=0.0000, Max=0.9932, Mean=0.0053\n",
      "2025-06-01 21:25:53,784 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0091\n",
      "2025-06-01 21:25:53,834 - INFO - dbz_final: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:53,875 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "2025-06-01 21:25:53,878 - INFO - Output tensor: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:53,927 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "2025-06-01 21:25:55,587 - INFO - Época 2 completada en 15.84s. Pérdida (train): 0.002196, Pérdida (val): 0.001731\n",
      "2025-06-01 21:25:55,801 - INFO - Mejor modelo guardado (Pérdida Val: 0.001731)\n",
      "2025-06-01 21:25:55,951 - INFO - Checkpoint guardado en la época 2\n",
      "2025-06-01 21:25:55,951 - INFO - Entrenamiento finalizado.\n",
      "2025-06-01 21:25:55,986 - INFO - Curvas de pérdida guardadas en /home/model_output_final_v_ckpt/loss_curves.png\n",
      "2025-06-01 21:25:55,988 - INFO - Modelo listo para predicción. Dtype: torch.float32, Dispositivo: cuda:0\n",
      "2025-06-01 21:25:55,988 - INFO - Generando predicciones de ejemplo...\n",
      "2025-06-01 21:25:56,134 - INFO - dbz_final: Min=0.0000, Max=0.9991, Mean=0.0055\n",
      "2025-06-01 21:25:56,138 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:56,216 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0054\n",
      "2025-06-01 21:25:56,223 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:56,293 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:56,305 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0086\n",
      "2025-06-01 21:25:56,368 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0052\n",
      "2025-06-01 21:25:56,381 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:56,441 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:56,458 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0094\n",
      "2025-06-01 21:25:56,518 - INFO - dbz_final: Min=0.0000, Max=0.9932, Mean=0.0053\n",
      "2025-06-01 21:25:56,541 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0091\n",
      "2025-06-01 21:25:56,594 - INFO - dbz_final: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:56,636 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "2025-06-01 21:25:56,644 - INFO - Output tensor: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:56,686 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "/opt/conda/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "2025-06-01 21:25:57,649 - INFO - Predicción Física Desnormalizada (muestra 0): Min=-39.91, Max=41.31, Mean=-inf\n",
      "/tmp/ipykernel_92/2418571061.py:601: RuntimeWarning: invalid value encountered in cast\n",
      "  np.clip(((pred_data_for_packing - output_add_offset) / output_scale_factor), -127, 127).round().astype(np.int8))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF predicción t+3min guardado: /home/predictions_final_v_ckpt/pred_DBZ_20250601_211056_sample0.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 21:25:58,473 - INFO - Predicción Física Desnormalizada (muestra 1): Min=-39.91, Max=41.38, Mean=-inf\n",
      "2025-06-01 21:25:58,549 - INFO - Proceso completado.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF predicción t+3min guardado: /home/predictions_final_v_ckpt/pred_DBZ_20250601_211057_sample1.nc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta # Asegúrate de importar timedelta\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from netCDF4 import Dataset as NCDataset # Renombrar para evitar conflicto con la clase Dataset\n",
    "# from torch.cuda.amp import autocast # Se usará torch.amp.autocast\n",
    "# from torch.cuda.amp import GradScaler # Se usará torch.amp.GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics # Para métricas adicionales como SSIM\n",
    "from torch.utils.checkpoint import checkpoint # <--- IMPORTANTE PARA GRADIENT CHECKPOINTING\n",
    "import torch.amp # <--- IMPORTANTE PARA APIS MODERNAS DE AMP\n",
    "\n",
    "# Configuración del Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuración para reproducibilidad y rendimiento\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    logging.info(f\"Semillas configuradas con valor: {seed}\")\n",
    "\n",
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, data_dir, subdirs_list, seq_len=6, pred_len=1,\n",
    "                 min_dbz=-30, max_dbz=70,\n",
    "                 expected_shape=(18, 500, 500), variable_name='DBZ'):\n",
    "        self.data_dir = data_dir\n",
    "        self.subdirs_list = subdirs_list\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.min_dbz = min_dbz\n",
    "        self.max_dbz = max_dbz\n",
    "        self.expected_z, self.expected_h, self.expected_w = expected_shape\n",
    "        self.variable_name = variable_name\n",
    "        self.valid_sequences = self._validate_subdirs()\n",
    "        if not self.valid_sequences:\n",
    "            logging.error(\"No se encontraron secuencias válidas. Verifica los datos y la estructura de carpetas.\")\n",
    "            raise ValueError(\"No se encontraron secuencias válidas.\")\n",
    "        logging.info(f\"RadarDataset inicializado con {len(self.valid_sequences)} secuencias válidas.\")\n",
    "\n",
    "    def _validate_subdirs(self):\n",
    "        valid_sequences = []\n",
    "        for subdir_name in self.subdirs_list:\n",
    "            subdir_path = os.path.join(self.data_dir, subdir_name)\n",
    "            if not os.path.isdir(subdir_path):\n",
    "                logging.warning(f\"Subdirectorio {subdir_name} no encontrado...\")\n",
    "                continue\n",
    "            if \".ipynb_checkpoints\" in subdir_name:\n",
    "                logging.debug(f\"Omitiendo directorio de checkpoints: {subdir_name}\")\n",
    "                continue\n",
    "            files = sorted(glob.glob(os.path.join(subdir_path, \"*.nc\")))\n",
    "            if len(files) >= self.seq_len + self.pred_len:\n",
    "                # Verificar que los archivos de salida tengan datos válidos\n",
    "                output_files = files[self.seq_len:self.seq_len + self.pred_len]\n",
    "                valid_output = True\n",
    "                for f in output_files:\n",
    "                    try:\n",
    "                        with NCDataset(f, 'r') as nc_file:\n",
    "                            if self.variable_name not in nc_file.variables:\n",
    "                                valid_output = False\n",
    "                                logging.warning(f\"Variable {self.variable_name} no encontrada en archivo de salida {f}\")\n",
    "                                break\n",
    "                            dbz_var = nc_file.variables[self.variable_name]\n",
    "                            data = dbz_var[0, ...] if dbz_var.ndim == 4 else dbz_var[...]\n",
    "                            logging.info(f\"Archivo {f}: Min={np.min(data):.2f}, Max={np.max(data):.2f}\")\n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Error leyendo archivo de salida {f}: {e}\")\n",
    "                        valid_output = False\n",
    "                        break\n",
    "                if valid_output:\n",
    "                    valid_sequences.append((files, subdir_name))\n",
    "            else:\n",
    "                logging.warning(f\"Subdirectorio {subdir_name} tiene {len(files)} archivos...\")\n",
    "        return valid_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_files, subdir_name = self.valid_sequences[idx]\n",
    "        input_data_list = []\n",
    "        output_data_list = []\n",
    "\n",
    "        all_files_for_sequence = sequence_files[:self.seq_len + self.pred_len]\n",
    "\n",
    "        for i, file_path in enumerate(all_files_for_sequence):\n",
    "            try:\n",
    "                with NCDataset(file_path, 'r') as nc_file:\n",
    "                    if self.variable_name not in nc_file.variables:\n",
    "                        logging.warning(f\"Variable '{self.variable_name}' no encontrada en {file_path}...\")\n",
    "                        return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "                    dbz_var = nc_file.variables[self.variable_name]\n",
    "                    if dbz_var.ndim == 4 and dbz_var.shape[0] == 1:\n",
    "                        data_raw = dbz_var[0, ...].astype(np.float32)\n",
    "                    elif dbz_var.ndim == 3:\n",
    "                        data_raw = dbz_var[...].astype(np.float32)\n",
    "                    else:\n",
    "                        logging.warning(f\"Forma de variable inesperada {dbz_var.shape}...\")\n",
    "                        return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "                    # Manejo de scale_factor, add_offset y _FillValue\n",
    "                    if hasattr(dbz_var, 'scale_factor') and hasattr(dbz_var, 'add_offset'):\n",
    "                        scale = dbz_var.scale_factor\n",
    "                        offset = dbz_var.add_offset\n",
    "                        fill_value_attr_packed = dbz_var._FillValue if hasattr(dbz_var, '_FillValue') else None\n",
    "                        dbz_physical = data_raw * scale + offset\n",
    "                        if fill_value_attr_packed is not None:\n",
    "                            is_fill_location = (data_raw == np.array(fill_value_attr_packed, dtype=data_raw.dtype))\n",
    "                            dbz_physical[is_fill_location] = np.nan  # Preservar como NaN\n",
    "                        dbz_for_norm = dbz_physical\n",
    "                    else:\n",
    "                        dbz_for_norm = data_raw\n",
    "                        if hasattr(dbz_var, '_FillValue'):\n",
    "                            is_fill_location = (dbz_for_norm == np.array(dbz_var._FillValue, dtype=dbz_for_norm.dtype))\n",
    "                            dbz_for_norm[is_fill_location] = np.nan\n",
    "\n",
    "                    # Definir rango correcto\n",
    "                    min_dbz = -29.0  # Basado en el archivo original\n",
    "                    max_dbz = 60.5\n",
    "                    # Clipping y Normalización, preservando NaN\n",
    "                    dbz_clipped = np.clip(dbz_for_norm, min_dbz, max_dbz, out=np.full_like(dbz_for_norm, np.nan))\n",
    "                    dbz_normalized = np.where(np.isnan(dbz_clipped), np.nan,\n",
    "                                            (dbz_clipped - min_dbz) / (max_dbz - min_dbz))\n",
    "                    \n",
    "                    # Verificar forma\n",
    "                    if dbz_normalized.shape != (self.expected_z, self.expected_h, self.expected_w):\n",
    "                        logging.warning(f\"Forma inesperada {dbz_normalized.shape}...\")\n",
    "                        return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "                    # Agregar canal\n",
    "                    dbz_final = dbz_normalized[..., np.newaxis]\n",
    "                    logging.info(f\"dbz_final: Min={np.nanmin(dbz_final):.4f}, Max={np.nanmax(dbz_final):.4f}, Mean={np.nanmean(dbz_final):.4f}\")\n",
    "\n",
    "                    if i < self.seq_len:\n",
    "                        input_data_list.append(dbz_final)\n",
    "                    else:\n",
    "                        output_data_list.append(dbz_final)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error procesando archivo {file_path}...: {e}\")\n",
    "                return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        if len(input_data_list) != self.seq_len or len(output_data_list) != self.pred_len:\n",
    "            logging.warning(f\"No se pudieron cargar suficientes frames...\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        input_tensor = np.stack(input_data_list, axis=1)  # (Z, T_in, H, W, C)\n",
    "        output_tensor = np.stack(output_data_list, axis=1)  # (Z, T_out, H, W, C)\n",
    "        logging.info(f\"Output tensor: Min={np.nanmin(output_tensor):.4f}, Max={np.nanmax(output_tensor):.4f}, Mean={np.nanmean(output_tensor):.4f}\")\n",
    "        # Evitar clipping agresivo\n",
    "        if np.any(output_tensor < 0, where=~np.isnan(output_tensor)) or np.any(output_tensor > 1, where=~np.isnan(output_tensor)):\n",
    "            logging.warning(f\"Datos objetivo normalizados fuera de rango [0,1]: Min={np.nanmin(output_tensor)}, Max={np.nanmax(output_tensor)}\")\n",
    "            # No clippear, en su lugar verificar el modelo\n",
    "        x = torch.from_numpy(np.nan_to_num(input_tensor, nan=0.0)).float()  # Convertir NaN a 0 para el modelo\n",
    "        y = torch.from_numpy(np.nan_to_num(output_tensor, nan=0.0)).float()\n",
    "        \n",
    "        # --- Lógica para devolver Timestamps (DEBES IMPLEMENTAR LA EXTRACCIÓN REAL) ---\n",
    "        # last_input_file_path = sequence_files[self.seq_len - 1]\n",
    "        # filename_no_ext = os.path.splitext(os.path.basename(last_input_file_path))[0]\n",
    "        # last_input_dt_utc_placeholder = datetime.utcnow() # ¡ESTO ES SOLO UN PLACEHOLDER!\n",
    "        # try:\n",
    "        #     # Intenta parsear el timestamp del nombre del archivo o del subdirectorio\n",
    "        #     # Ejemplo: parts = filename_no_ext.split('_'); timestamp_str = parts[0][-8:] + parts[1]\n",
    "        #     # last_input_dt_utc = datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")\n",
    "        #     pass # Implementa tu lógica de parseo aquí\n",
    "        # except Exception as e_time:\n",
    "        #     logging.warning(f\"No se pudo parsear el timestamp de {last_input_file_path} en dataset. Usando placeholder. Error: {e_time}\")\n",
    "        #     # last_input_dt_utc = last_input_dt_utc_placeholder # Mantener el placeholder si falla\n",
    "    \n",
    "        # return x, y, last_input_dt_utc_placeholder # Si devuelves timestamp\n",
    "        return x, y # Si NO devuelves timestamp por ahora\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "        if self.bias:\n",
    "            nn.init.zeros_(self.conv.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "    def init_hidden(self, batch_size, image_size, device):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=device))\n",
    "\n",
    "class ConvLSTM2DLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM2DLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "        self.cell = ConvLSTMCell(input_dim, hidden_dim, kernel_size, bias)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None): # input_tensor: (B, T_in, C_in, H, W)\n",
    "        b, seq_len, _, h, w = input_tensor.size() # _ es C_in\n",
    "        device = input_tensor.device\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self.cell.init_hidden(b, (h, w), device)\n",
    "\n",
    "        layer_output_list = []\n",
    "        h_cur, c_cur = hidden_state\n",
    "        for t in range(seq_len):\n",
    "            h_cur, c_cur = self.cell(input_tensor=input_tensor[:, t, :, :, :], cur_state=[h_cur, c_cur])\n",
    "            layer_output_list.append(h_cur)\n",
    "\n",
    "        if self.return_all_layers:\n",
    "            layer_output = torch.stack(layer_output_list, dim=1) # (B, T_in, C_hidden, H, W)\n",
    "        else:\n",
    "            # Solo el último estado oculto como salida de la capa, pero manteniendo la dim de tiempo\n",
    "            layer_output = h_cur.unsqueeze(1) # (B, 1, C_hidden, H, W)\n",
    "\n",
    "        return layer_output, (h_cur, c_cur)\n",
    "\n",
    "class ConvLSTM3D_Enhanced(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dims=[32, 64], kernel_sizes=[(3,3), (3,3)],\n",
    "                 num_layers=2, pred_steps=1, use_layer_norm=True, use_residual=False,\n",
    "                 img_height=500, img_width=500):\n",
    "        super(ConvLSTM3D_Enhanced, self).__init__()\n",
    "        if isinstance(hidden_dims, int): hidden_dims = [hidden_dims] * num_layers\n",
    "        if isinstance(kernel_sizes, tuple): kernel_sizes = [kernel_sizes] * num_layers\n",
    "        assert len(hidden_dims) == num_layers and len(kernel_sizes) == num_layers\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.num_layers = num_layers\n",
    "        self.pred_steps = pred_steps\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.use_residual = use_residual\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList() if use_layer_norm else None\n",
    "\n",
    "        current_dim = input_dim\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(\n",
    "                ConvLSTM2DLayer(input_dim=current_dim, hidden_dim=hidden_dims[i],\n",
    "                                kernel_size=kernel_sizes[i], bias=True,\n",
    "                                return_all_layers=True if i < num_layers - 1 else False)\n",
    "            )\n",
    "            if use_layer_norm:\n",
    "                self.layer_norms.append(nn.LayerNorm([hidden_dims[i], img_height, img_width]))\n",
    "            current_dim = hidden_dims[i]\n",
    "\n",
    "        self.output_conv = nn.Conv3d(in_channels=hidden_dims[-1],\n",
    "                                     out_channels=input_dim * pred_steps,\n",
    "                                     kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.output_conv.weight)\n",
    "        nn.init.zeros_(self.output_conv.bias)\n",
    "\n",
    "        logging.info(f\"Modelo ConvLSTM3D_Enhanced creado: {num_layers} capas, Hidden dims: {hidden_dims}, LayerNorm: {use_layer_norm}, PredSteps: {pred_steps}\")\n",
    "\n",
    "    def forward(self, x_volumetric):  # Espera (Z, B, T_in, H, W, C_in)\n",
    "        num_z_levels, b, seq_len, h, w, c_in = x_volumetric.shape\n",
    "        all_level_predictions = []\n",
    "\n",
    "        for z_idx in range(num_z_levels):\n",
    "            x_level = x_volumetric[z_idx, ...]  # (B, T_in, H, W, C_in)\n",
    "            x_level_permuted = x_level.permute(0, 1, 4, 2, 3)  # (B, T_in, C_in, H, W)\n",
    "            current_input = x_level_permuted\n",
    "\n",
    "            hidden_states_for_level = [None] * self.num_layers\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                layer_output, hidden_state = self.layers[i](current_input, hidden_states_for_level[i])\n",
    "                hidden_states_for_level[i] = hidden_state\n",
    "\n",
    "                if self.use_layer_norm and self.layer_norms:\n",
    "                    B_ln, T_ln, C_ln, H_ln, W_ln = layer_output.shape\n",
    "                    output_reshaped_for_ln = layer_output.contiguous().view(B_ln * T_ln, C_ln, H_ln, W_ln)\n",
    "                    normalized_output = self.layer_norms[i](output_reshaped_for_ln)\n",
    "                    layer_output = normalized_output.view(B_ln, T_ln, C_ln, H_ln, W_ln)\n",
    "                current_input = layer_output\n",
    "\n",
    "            output_for_conv3d = current_input.permute(0, 2, 1, 3, 4)\n",
    "            raw_conv_output = self.output_conv(output_for_conv3d)\n",
    "            prediction_features = raw_conv_output.squeeze(2)\n",
    "            level_prediction = prediction_features.view(b, self.pred_steps, self.input_dim, h, w)\n",
    "            level_prediction = level_prediction.permute(0, 1, 3, 4, 2)\n",
    "            level_prediction = self.sigmoid(level_prediction)\n",
    "            all_level_predictions.append(level_prediction)\n",
    "\n",
    "        predictions_volumetric = torch.stack(all_level_predictions, dim=0)\n",
    "        return predictions_volumetric\n",
    "\n",
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, data_range=1.0, kernel_size_for_metric=7):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        # No necesitas try-except aquí si estás usando una versión de torchmetrics que lo soporta\n",
    "        self.ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure(\n",
    "            data_range=data_range,\n",
    "            kernel_size=kernel_size_for_metric,\n",
    "            reduction='elementwise_mean' # Común, o None y luego .mean()\n",
    "        ).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def forward(self, img1, img2): # Espera (Z, B, T_pred, H, W, C)\n",
    "        num_z, batch_s, pred_t, height, width, channels = img1.shape\n",
    "\n",
    "        # SSIM se aplica típicamente a imágenes (o slices 2D/3D con un canal)\n",
    "        # Aplanar Z, B, T_pred en la dimensión de batch para SSIM\n",
    "        # Permutar para tener (Batch_flat, Canales, H, W)\n",
    "        img1_reshaped = img1.permute(0, 1, 2, 5, 3, 4).contiguous().view(-1, channels, height, width)\n",
    "        img2_reshaped = img2.permute(0, 1, 2, 5, 3, 4).contiguous().view(-1, channels, height, width)\n",
    "\n",
    "        ssim_val_elementwise = self.ssim_metric(img1_reshaped, img2_reshaped) # Esto dará un valor por imagen en el batch aplanado\n",
    "        ssim_val_mean = ssim_val_elementwise.mean() # Tomar la media sobre todos los elementos del batch aplanado\n",
    "        logging.info(f\"SSIM Mean: {ssim_val_mean.item():.4f}\")\n",
    "        return 1.0 - ssim_val_mean # Queremos maximizar SSIM, así que minimizamos 1-SSIM\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Usando dispositivo: {device}\")\n",
    "    model.to(device)  # Mover el modelo al dispositivo ANTES de crear el optimizador\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config.get('weight_decay', 1e-5))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=config.get('lr_patience', 3), verbose=True)\n",
    "\n",
    "    criterion_mse = nn.MSELoss().to(device)\n",
    "    criterion_ssim = None\n",
    "    ssim_loss_weight = 0.0\n",
    "    mse_loss_weight = 1.0\n",
    "\n",
    "    if config.get('use_ssim_loss', False):\n",
    "        try:\n",
    "            criterion_ssim = SSIMLoss(\n",
    "                data_range=1.0,  # Ya que los datos están normalizados a [0,1]\n",
    "                kernel_size_for_metric=config.get('ssim_kernel_size', 7)\n",
    "            ).to(device)  # SSIMLoss ya se mueve al device en su __init__\n",
    "            ssim_loss_weight = config.get('ssim_loss_weight', 0.3)\n",
    "            mse_loss_weight = 1.0 - ssim_loss_weight\n",
    "            logging.info(f\"Usando SSIM loss con peso {ssim_loss_weight} y MSE con peso {mse_loss_weight}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al inicializar SSIMLoss: {e}. Se usará solo MSE.\")\n",
    "            criterion_ssim = None  # Reasegurar\n",
    "            ssim_loss_weight = 0.0\n",
    "            mse_loss_weight = 1.0\n",
    "\n",
    "    scaler = torch.amp.GradScaler(enabled=config['use_amp'])\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "    accumulation_steps = config.get('accumulation_steps', 1)\n",
    "\n",
    "    logging.info(f\"Iniciando entrenamiento: {config['epochs']} épocas, LR: {config['learning_rate']}, Batch (efectivo): {config['batch_size'] * accumulation_steps}\")\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        optimizer.zero_grad()  # Mover zero_grad aquí, antes del bucle de acumulación\n",
    "        # Monitoreo de uso de GPU\n",
    "        if torch.cuda.is_available():\n",
    "            logging.info(f\"Inicio Época {epoch+1} - Memoria GPU Asignada: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB, Reservada: {torch.cuda.memory_reserved(device) / 1024**3:.2f} GB\")\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):  # x, y de RadarDataset son (Z, T, H, W, C)\n",
    "            # DataLoader añade B: (B, Z, T, H, W, C)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Permutar para el modelo: (Z, B, T, H, W, C)\n",
    "            if x.dim() == 6 and y.dim() == 6:  # (B,Z,T_in,H,W,C) y (B,Z,T_out,H,W,C)\n",
    "                x = x.permute(1, 0, 2, 3, 4, 5)\n",
    "                y = y.permute(1, 0, 2, 3, 4, 5)\n",
    "            else:\n",
    "                logging.error(f\"Formas inesperadas para x o y antes de la permutación: x={x.shape}, y={y.shape}\")\n",
    "                continue\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=config['use_amp']):\n",
    "                predictions = model(x)  # Espera (Z,B,T_in,H,W,C) -> Sale (Z,B,T_pred,H,W,C)\n",
    "\n",
    "                # Asegúrate que 'y' (objetivo) tenga la misma forma que 'predictions'\n",
    "                if predictions.shape != y.shape:\n",
    "                    logging.error(f\"Discrepancia de formas entre predicción {predictions.shape} y objetivo {y.shape}\")\n",
    "                    continue\n",
    "\n",
    "                loss_mse_val = criterion_mse(predictions, y)\n",
    "                current_loss = loss_mse_val\n",
    "\n",
    "                if criterion_ssim is not None:\n",
    "                    loss_ssim_component = criterion_ssim(predictions, y)\n",
    "                    current_loss = mse_loss_weight * loss_mse_val + ssim_loss_weight * loss_ssim_component\n",
    "\n",
    "                # Loguear métricas del primer batch de la primera época\n",
    "                if batch_idx == 0 and epoch == 0:\n",
    "                    logging.info(f\"  Predicciones (normalizadas): Min={predictions.min().item():.4f}, Max={predictions.max().item():.4f}, Mean={predictions.mean().item():.4f}\")\n",
    "                    logging.info(f\"  Objetivos y (normalizados): Min={y.min().item():.4f}, Max={y.max().item():.4f}, Mean={y.mean().item():.4f}\")\n",
    "\n",
    "                loss_to_accumulate = current_loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss_to_accumulate).backward()\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                if config.get('clip_grad_norm', None):\n",
    "                    scaler.unscale_(optimizer)  # Unscale antes de clip_grad_norm\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['clip_grad_norm'])\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()  # Zero grad después de step y update\n",
    "\n",
    "            running_train_loss += current_loss.item()  # Usar current_loss (no dividida por accumulation_steps) para el log\n",
    "\n",
    "            if (batch_idx + 1) % config.get('log_interval', 1) == 0:\n",
    "                logging.info(f\"Época {epoch+1}/{config['epochs']} [{batch_idx+1}/{len(train_loader)}] - Pérdida (batch): {current_loss.item():.6f}\")\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validación\n",
    "        if val_loader and len(val_loader) > 0:  # Asegurar que val_loader no sea None y tenga datos\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    if x_val.dim() == 6 and y_val.dim() == 6:\n",
    "                        x_val = x_val.permute(1, 0, 2, 3, 4, 5)\n",
    "                        y_val = y_val.permute(1, 0, 2, 3, 4, 5)\n",
    "                    else:\n",
    "                        logging.error(f\"Formas inesperadas (val) x={x_val.shape}, y={y_val.shape}\")\n",
    "                        continue\n",
    "\n",
    "                    with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=config['use_amp']):\n",
    "                        predictions_val = model(x_val)\n",
    "                        if predictions_val.shape != y_val.shape:\n",
    "                            logging.error(f\"Discrepancia de formas (val) entre predicción {predictions_val.shape} y objetivo {y_val.shape}\")\n",
    "                            continue\n",
    "                        val_loss_mse_val = criterion_mse(predictions_val, y_val)\n",
    "                        current_val_loss = val_loss_mse_val\n",
    "                        if criterion_ssim is not None:\n",
    "                            val_loss_ssim_component = criterion_ssim(predictions_val, y_val)\n",
    "                            current_val_loss = mse_loss_weight * val_loss_mse_val + ssim_loss_weight * val_loss_ssim_component\n",
    "                    running_val_loss += current_val_loss.item()\n",
    "\n",
    "            if len(val_loader) > 0:  # Evitar división por cero\n",
    "                avg_val_loss = running_val_loss / len(val_loader)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                scheduler.step(avg_val_loss)\n",
    "                epoch_duration = time.time() - epoch_start_time\n",
    "                logging.info(f\"Época {epoch+1} completada en {epoch_duration:.2f}s. Pérdida (train): {avg_train_loss:.6f}, Pérdida (val): {avg_val_loss:.6f}\")\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    torch.save({'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(), 'loss': best_val_loss},\n",
    "                               os.path.join(config['model_save_dir'], \"best_convlstm_model.pth\"))\n",
    "                    logging.info(f\"Mejor modelo guardado (Pérdida Val: {best_val_loss:.6f})\")\n",
    "            else:  # Si len(val_loader) es 0\n",
    "                epoch_duration = time.time() - epoch_start_time\n",
    "                logging.info(f\"Época {epoch+1} completada en {epoch_duration:.2f}s. Pérdida (train): {avg_train_loss:.6f} (Dataset de validación vacío, no se calculó pérdida de validación)\")\n",
    "        else:  # Si no hay val_loader\n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            logging.info(f\"Época {epoch+1} completada en {epoch_duration:.2f}s. Pérdida (train): {avg_train_loss:.6f} (No hay val_loader)\")\n",
    "\n",
    "        # Guardar checkpoint de época\n",
    "        if (epoch + 1) % config.get('checkpoint_interval', 1) == 0:\n",
    "            torch.save({'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(), 'train_losses': train_losses,\n",
    "                        'val_losses': val_losses if (val_loader and len(val_loader) > 0) else []},\n",
    "                       os.path.join(config['model_save_dir'], f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
    "            logging.info(f\"Checkpoint guardado en la época {epoch+1}\")\n",
    "\n",
    "    logging.info(\"Entrenamiento finalizado.\")\n",
    "    if train_loader and len(train_losses) > 0:  # Solo plotear si hubo entrenamiento y pérdidas\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Pérdida Entrenamiento')\n",
    "        if val_loader and len(val_losses) > 0:\n",
    "            plt.plot(val_losses, label='Pérdida Validación')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Pérdida')\n",
    "        plt.legend()\n",
    "        plt.title('Curvas de Pérdida del Entrenamiento')\n",
    "        plt.savefig(os.path.join(config['model_save_dir'], \"loss_curves.png\"))\n",
    "        plt.close()\n",
    "        logging.info(f\"Curvas de pérdida guardadas en {os.path.join(config['model_save_dir'], 'loss_curves.png')}\")\n",
    "\n",
    "    return model, {'train_losses': train_losses, 'val_losses': val_losses if (val_loader and len(val_loader) > 0) else []}\n",
    "\n",
    "def generate_prediction_netcdf(model, data_loader, config, device, num_samples=1):\n",
    "    model.to(device)\n",
    "    model.float()\n",
    "    model.eval()\n",
    "\n",
    "    output_dir = config['predictions_output_dir']\n",
    "    min_dbz_model_output = config['min_dbz']  # e.g., -29\n",
    "    max_dbz_model_output = config['max_dbz']  # e.g., 60.5\n",
    "    \n",
    "    output_scale_factor = np.float32(config.get('output_nc_scale_factor', 0.5))\n",
    "    output_add_offset = np.float32(config.get('output_nc_add_offset', 33.5))\n",
    "    output_fill_value_byte = np.int8(-128)\n",
    "    \n",
    "    time_dim_name = config.get('time_dim_name', 'time')\n",
    "    bounds_dim_name = config.get('bounds_dim_name', 'bounds')\n",
    "    x_dim_name = config.get('x_dim_name', 'x0')\n",
    "    y_dim_name = config.get('y_dim_name', 'y0')\n",
    "    z_dim_name = config.get('z_dim_name', 'z0')\n",
    "    \n",
    "    time_var_name = config.get('time_var_name', 'time')\n",
    "    time_bounds_var_name = config.get('time_bounds_var_name', 'time_bounds')\n",
    "    start_time_var_name = config.get('start_time_var_name', 'start_time')\n",
    "    stop_time_var_name = config.get('stop_time_var_name', 'stop_time')\n",
    "    x_coord_var_name = config.get('x_coord_var_name', 'x0')\n",
    "    y_coord_var_name = config.get('y_coord_var_name', 'y0')\n",
    "    z_coord_var_name = config.get('z_coord_var_name', 'z0')\n",
    "    \n",
    "    grid_mapping_var_name = config.get('projection_variable_name', \"grid_mapping_0\")\n",
    "    dbz_var_name = config.get('dbz_variable_name_pred_nc', 'DBZ')\n",
    "\n",
    "    proj_origin_lon = config.get('sensor_longitude', -68.0169982910156)\n",
    "    proj_origin_lat = config.get('sensor_latitude', -34.6479988098145)\n",
    "    earth_radius_m = config.get('earth_radius_m', 6378137)\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    num_z = config['expected_shape'][0]\n",
    "    num_y = config['expected_shape'][1]\n",
    "    num_x = config['expected_shape'][2]\n",
    "\n",
    "    z_coord_values = np.arange(config['grid_minz_km'], config['grid_minz_km'] + num_z * config['grid_dz_km'], config['grid_dz_km'], dtype=np.float32)[:num_z]\n",
    "    x_coord_values = np.arange(config['grid_minx_km'], config['grid_minx_km'] + num_x * config['grid_dx_km'], config['grid_dx_km'], dtype=np.float32)[:num_x]\n",
    "    y_coord_values = np.arange(config['grid_miny_km'], config['grid_miny_km'] + num_y * config['grid_dy_km'], config['grid_dy_km'], dtype=np.float32)[:num_y]\n",
    "    \n",
    "    # Calcular lat0 y lon0\n",
    "    import pyproj\n",
    "    proj = pyproj.Proj(proj=\"aeqd\", lon_0=proj_origin_lon, lat_0=proj_origin_lat, R=earth_radius_m)\n",
    "    x_grid, y_grid = np.meshgrid(x_coord_values, y_coord_values)\n",
    "    lon0, lat0 = proj(x_grid, y_grid, inverse=True)\n",
    "\n",
    "    sample_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data_batch in enumerate(data_loader):\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "            x_input_volume, y_true_volume = data_batch\n",
    "            last_input_dt_from_loader = datetime.utcnow() - timedelta(minutes=config['seq_len'] * config['prediction_interval_minutes'])\n",
    "\n",
    "            x_permuted = x_input_volume.permute(1, 0, 2, 3, 4, 5)\n",
    "            x_to_model = x_permuted.to(device)\n",
    "            \n",
    "            if x_to_model.shape[1] > 1:\n",
    "                logging.warning(f\"Procesando solo el primer item de un batch de tamaño {x_to_model.shape[1]}\")\n",
    "            current_x_to_model = x_to_model[:, 0:1, ...]\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=config['use_amp']):\n",
    "                prediction_norm_all_steps = model(current_x_to_model)\n",
    "            \n",
    "            pred_data_np = prediction_norm_all_steps[:, 0, 0, :, :, 0].cpu().numpy()  # (Z, H, W)\n",
    "            \n",
    "            # Desnormalizar, preservando NaN\n",
    "            pred_data_desnorm_float = np.where(np.isnan(pred_data_np), np.nan,\n",
    "                                              pred_data_np * (max_dbz_model_output - min_dbz_model_output) + min_dbz_model_output)\n",
    "            \n",
    "            logging.info(f\"Predicción Física Desnormalizada (muestra {sample_count}): Min={np.nanmin(pred_data_desnorm_float):.2f}, Max={np.nanmax(pred_data_desnorm_float):.2f}, Mean={np.nanmean(pred_data_desnorm_float):.2f}\")\n",
    "            \n",
    "            # Marcar áreas sin reflectividad como _FillValue\n",
    "            pred_data_for_packing = np.where(pred_data_desnorm_float < 0, np.nan, pred_data_desnorm_float)\n",
    "            pred_data_byte = np.where(np.isnan(pred_data_for_packing), output_fill_value_byte,\n",
    "                                     np.clip(((pred_data_for_packing - output_add_offset) / output_scale_factor), -127, 127).round().astype(np.int8))\n",
    "            pred_data_final_for_nc = np.expand_dims(pred_data_byte, axis=0)\n",
    "\n",
    "            last_input_frame_datetime_utc = last_input_dt_from_loader.replace(tzinfo=None)\n",
    "            forecast_lead_seconds = (0 + 1) * config['prediction_interval_minutes'] * 60\n",
    "            actual_forecast_datetime_utc = last_input_frame_datetime_utc + timedelta(seconds=forecast_lead_seconds)\n",
    "\n",
    "            epoch_time = datetime(1970, 1, 1, 0, 0, 0)\n",
    "            time_value_seconds = (actual_forecast_datetime_utc - epoch_time).total_seconds()\n",
    "            time_begin_calc_seconds = time_value_seconds - (2 * 60 + 48)\n",
    "            time_end_calc_seconds = time_value_seconds\n",
    "            \n",
    "            file_timestamp_str = actual_forecast_datetime_utc.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_filename = os.path.join(output_dir, f\"pred_{dbz_var_name}_{file_timestamp_str}_sample{sample_count}.nc\")\n",
    "\n",
    "            with NCDataset(output_filename, 'w', format='NETCDF3_CLASSIC') as ncfile:\n",
    "                ncfile.Conventions = \"CF-1.6\"\n",
    "                ncfile.title = f\"{config.get('radar_name', 'SAN_RAFAEL')} - Forecast t+{config['prediction_interval_minutes']}min\"\n",
    "                ncfile.institution = config.get('institution_name', \"UCAR\")\n",
    "                ncfile.source = config.get('data_source_name', \"Gobierno de Mendoza\")\n",
    "                ncfile.history = f\"Created {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')} by ConvLSTM prediction script.\"\n",
    "                ncfile.comment = f\"Forecast data from ConvLSTM model for lead time +{forecast_lead_seconds/60.0:.0f} min.\"\n",
    "                ncfile.references = f\"Tesis de {config.get('author_name', 'Federico Caballero')}, {config.get('author_institution', 'Universidad de Mendoza')}\"\n",
    "\n",
    "                ncfile.createDimension(time_dim_name, None)\n",
    "                ncfile.createDimension(bounds_dim_name, 2)\n",
    "                ncfile.createDimension(x_dim_name, num_x)\n",
    "                ncfile.createDimension(y_dim_name, num_y)\n",
    "                ncfile.createDimension(z_dim_name, num_z)\n",
    "\n",
    "                time_v = ncfile.createVariable(time_var_name, 'f8', (time_dim_name,))\n",
    "                time_v.standard_name = \"time\"\n",
    "                time_v.long_name = \"Data time\"\n",
    "                time_v.units = \"seconds since 1970-01-01T00:00:00Z\"\n",
    "                time_v.axis = \"T\"\n",
    "                time_v.bounds = time_bounds_var_name\n",
    "                time_v.comment = actual_forecast_datetime_utc.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_v[:] = [time_value_seconds]\n",
    "\n",
    "                time_bnds_v = ncfile.createVariable(time_bounds_var_name, 'f8', (time_dim_name, bounds_dim_name))\n",
    "                time_bnds_v.comment = \"time_bounds also stored the start and stop times, provided the time variable value lies within the start_time to stop_time interval\"\n",
    "                time_bnds_v.units = \"seconds since 1970-01-01T00:00:00Z\"\n",
    "                time_bnds_v[:] = [[time_begin_calc_seconds, time_end_calc_seconds]]\n",
    "\n",
    "                start_time_v = ncfile.createVariable(start_time_var_name, 'f8', (time_dim_name,))\n",
    "                start_time_v.long_name = \"start_time\"\n",
    "                start_time_v.units = \"seconds since 1970-01-01T00:00:00Z\"\n",
    "                start_time_v.comment = datetime.fromtimestamp(time_begin_calc_seconds).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                start_time_v[:] = [time_begin_calc_seconds]\n",
    "\n",
    "                stop_time_v = ncfile.createVariable(stop_time_var_name, 'f8', (time_dim_name,))\n",
    "                stop_time_v.long_name = \"stop_time\"\n",
    "                stop_time_v.units = \"seconds since 1970-01-01T00:00:00Z\"\n",
    "                stop_time_v.comment = datetime.fromtimestamp(time_end_calc_seconds).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                stop_time_v[:] = [time_end_calc_seconds]\n",
    "\n",
    "                x_v = ncfile.createVariable(x_coord_var_name, 'f4', (x_dim_name,))\n",
    "                x_v.standard_name = \"projection_x_coordinate\"\n",
    "                x_v.units = \"km\"\n",
    "                x_v.axis = \"X\"\n",
    "                x_v[:] = x_coord_values\n",
    "                \n",
    "                y_v = ncfile.createVariable(y_coord_var_name, 'f4', (y_dim_name,))\n",
    "                y_v.standard_name = \"projection_y_coordinate\"\n",
    "                y_v.units = \"km\"\n",
    "                y_v.axis = \"Y\"\n",
    "                y_v[:] = y_coord_values\n",
    "                \n",
    "                z_v = ncfile.createVariable(z_coord_var_name, 'f4', (z_dim_name,))\n",
    "                z_v.standard_name = \"altitude\"\n",
    "                z_v.long_name = \"constant altitude levels\"\n",
    "                z_v.units = \"km\"\n",
    "                z_v.positive = \"up\"\n",
    "                z_v.axis = \"Z\"\n",
    "                z_v[:] = z_coord_values\n",
    "\n",
    "                # Agregar lat0 y lon0\n",
    "                lat0_v = ncfile.createVariable('lat0', 'f4', (y_dim_name, x_dim_name))\n",
    "                lat0_v.standard_name = \"latitude\"\n",
    "                lat0_v.units = \"degrees_north\"\n",
    "                lat0_v[:] = lat0\n",
    "                \n",
    "                lon0_v = ncfile.createVariable('lon0', 'f4', (y_dim_name, x_dim_name))\n",
    "                lon0_v.standard_name = \"longitude\"\n",
    "                lon0_v.units = \"degrees_east\"\n",
    "                lon0_v[:] = lon0\n",
    "\n",
    "                gm_v = ncfile.createVariable(grid_mapping_var_name, 'i4')\n",
    "                gm_v.grid_mapping_name = \"azimuthal_equidistant\"\n",
    "                gm_v.longitude_of_projection_origin = proj_origin_lon\n",
    "                gm_v.latitude_of_projection_origin = proj_origin_lat\n",
    "                gm_v.false_easting = 0.0\n",
    "                gm_v.false_northing = 0.0\n",
    "                gm_v.earth_radius = earth_radius_m\n",
    "\n",
    "                dbz_v = ncfile.createVariable(dbz_var_name, 'i1', (time_dim_name, z_dim_name, y_dim_name, x_dim_name),\n",
    "                                             fill_value=output_fill_value_byte)\n",
    "                dbz_v.units = 'dBZ'\n",
    "                dbz_v.long_name = 'DBZ'\n",
    "                dbz_v.standard_name = 'DBZ'\n",
    "                dbz_v.coordinates = \"lon0 lat0\"\n",
    "                dbz_v.grid_mapping = grid_mapping_var_name\n",
    "                dbz_v.scale_factor = output_scale_factor\n",
    "                dbz_v.add_offset = output_add_offset\n",
    "                dbz_v.valid_min = np.int8(-127)\n",
    "                dbz_v.valid_max = np.int8(127)\n",
    "                dbz_v.min_value = np.float32(config.get('template_min_value', -29.0))\n",
    "                dbz_v.max_value = np.float32(config.get('template_max_value', 60.5))\n",
    "                dbz_v[:] = pred_data_final_for_nc\n",
    "\n",
    "                print(f\"NetCDF predicción t+{config['prediction_interval_minutes']}min guardado: {output_filename}\")\n",
    "            sample_count += 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_seed(42)\n",
    "    config = {\n",
    "        'data_dir': \"/home/sample\", # AJUSTADO para tu prueba con el tar.gz\n",
    "        'model_save_dir': \"/home/model_output_final_v_ckpt\",\n",
    "        'predictions_output_dir': \"/home/predictions_final_v_ckpt\",\n",
    "\n",
    "        'seq_len': 6,      # MODIFICADO\n",
    "        'pred_len': 1,      # MODIFICADO (RadarDataset usará esto para y_true_volume)\n",
    "        'pred_steps_model': 1, # MODIFICADO (El modelo generará esta cantidad de pasos)\n",
    "\n",
    "        'min_dbz': -40.0,\n",
    "        'max_dbz': 70.0,\n",
    "        'fill_value': -9999.0, # Fill value para los datos flotantes del modelo ANTES de empaquetar\n",
    "        'expected_shape': (18, 500, 500), # nz, ny, nx\n",
    "        'dbz_variable_name': 'DBZ',\n",
    "        # 'dbz_variable_name_pred': 'DBZ_forecast', # Usaremos dbz_variable_name_pred_nc para el nombre final\n",
    "\n",
    "        # --- Parámetros del Sensor y Grilla (para coincidir con la plantilla MDV/NetCDF) ---\n",
    "        'sensor_latitude': -34.64799880981445,    # De la plantilla\n",
    "        'sensor_longitude': -68.01699829101562,   # De la plantilla\n",
    "        'sensor_altitude_km': 0.550000011920929, # De la plantilla\n",
    "\n",
    "        # Grilla de la plantilla (18, 500, 500)\n",
    "        'grid_minz_km': 1.0,\n",
    "        'grid_dz_km': 1.0,\n",
    "        'grid_minx_km': -249.5,\n",
    "        'grid_dx_km': 1.0,\n",
    "        'grid_miny_km': -249.5,\n",
    "        'grid_dy_km': 1.0,\n",
    "\n",
    "        'radar_name': \"SAN_RAFAEL\",             # De la plantilla\n",
    "        'institution_name': \"UCAR\",             # De la plantilla\n",
    "        'author_name': \"Federico Caballero\",\n",
    "        'author_institution': \"Universidad de Mendoza\",\n",
    "        'data_source_name': \"Gobierno de Mendoza\", # De la plantilla (para atributo global 'source')\n",
    "\n",
    "        # --- Parámetros para la SALIDA NetCDF (para que se parezca a la plantilla) ---\n",
    "        'dbz_variable_name_pred_nc': 'DBZ', # Nombre de la variable en el NetCDF de SALIDA\n",
    "        # Parámetros de empaquetado para la variable DBZ de SALIDA (byte)\n",
    "        # Tomados de la plantilla ncfdata20100101_204855_ncdump.txt\n",
    "        'output_nc_scale_factor': 0.5,\n",
    "        'output_nc_add_offset': 33.5,\n",
    "        # Parámetros de la proyección AZIMUTHAL_EQUIDISTANT (de la plantilla)\n",
    "        'projection_variable_name': \"grid_mapping_0\",\n",
    "        'earth_radius_m': 6378137.0,\n",
    "\n",
    "        'prediction_interval_minutes': 3, # Intervalo entre los archivos de entrada/salida\n",
    "                                          # Si tus archivos de radar son cada 3 min, este es el valor.\n",
    "\n",
    "        'model_input_dim': 1,\n",
    "        'model_hidden_dims': [32, 32],\n",
    "        'model_kernel_sizes': [(3,3), (3,3)],\n",
    "        'model_num_layers': 2,\n",
    "        'model_use_layer_norm': True, 'model_use_residual': False,\n",
    "\n",
    "        'batch_size': 1,\n",
    "        'epochs': 2, # AJUSTADO para una prueba rápida de predicción\n",
    "        'learning_rate': 1e-3, 'weight_decay': 1e-4, 'lr_patience': 3,\n",
    "        'use_amp': True, 'accumulation_steps': 1,\n",
    "        'clip_grad_norm': 1.0,\n",
    "        'log_interval': 1, # Loguear cada batch para prueba\n",
    "        'checkpoint_interval': 1,\n",
    "\n",
    "        'use_ssim_loss': False, 'ssim_kernel_size': 7, 'ssim_loss_weight': 0.3,\n",
    "\n",
    "        'train_val_split_ratio': 0.8,\n",
    "        'max_sequences_to_use': 10, # AJUSTADO para una prueba rápida con tus datos de muestra\n",
    "    }\n",
    "\n",
    "    os.makedirs(config['model_save_dir'], exist_ok=True)\n",
    "    os.makedirs(config['predictions_output_dir'], exist_ok=True)\n",
    "\n",
    "    all_subdirs_available = sorted([\n",
    "        d for d in os.listdir(config['data_dir'])\n",
    "        if os.path.isdir(os.path.join(config['data_dir'], d)) and not d.startswith('.')\n",
    "    ])\n",
    "    if not all_subdirs_available: logging.error(f\"No subdirs in {config['data_dir']}\"); return\n",
    "\n",
    "    if config['max_sequences_to_use'] and config['max_sequences_to_use'] < len(all_subdirs_available):\n",
    "        logging.info(f\"Usando muestra aleatoria de {config['max_sequences_to_use']} secuencias.\")\n",
    "        random.shuffle(all_subdirs_available)\n",
    "        subdirs_to_use = all_subdirs_available[:config['max_sequences_to_use']]\n",
    "    else: subdirs_to_use = all_subdirs_available\n",
    "    logging.info(f\"Total secuencias a usar: {len(subdirs_to_use)}.\")\n",
    "    if not subdirs_to_use : logging.error(\"No hay secuencias para procesar.\"); return\n",
    "\n",
    "    split_idx = int(len(subdirs_to_use) * config['train_val_split_ratio'])\n",
    "    train_subdirs, val_subdirs = subdirs_to_use[:split_idx], subdirs_to_use[split_idx:]\n",
    "    if not train_subdirs: logging.info(\"No hay secuencias de entrenamiento, usando todas para validación si existen.\"); train_subdirs = [] # Permitir correr solo con validación para predicción\n",
    "\n",
    "    logging.info(f\"Entrenamiento: {len(train_subdirs)} sec. Validación: {len(val_subdirs)} sec.\")\n",
    "\n",
    "    train_loader = None\n",
    "    if train_subdirs:\n",
    "        train_dataset = RadarDataset(config['data_dir'], train_subdirs, seq_len=config['seq_len'], pred_len=config['pred_len'],\n",
    "                                     min_dbz=config['min_dbz'], max_dbz=config['max_dbz'],\n",
    "                                     expected_shape=config['expected_shape'], variable_name=config['dbz_variable_name'])\n",
    "        if len(train_dataset) > 0:\n",
    "            train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "        else:\n",
    "            logging.info(\"Dataset de entrenamiento vacío después de filtrar.\")\n",
    "            # Si no hay datos de entrenamiento, no podemos entrenar. Podrías querer cargar un modelo directamente.\n",
    "\n",
    "    val_loader = None; val_dataset_len = 0\n",
    "    if val_subdirs:\n",
    "        val_dataset = RadarDataset(config['data_dir'], val_subdirs, seq_len=config['seq_len'], pred_len=config['pred_len'],\n",
    "                                   min_dbz=config['min_dbz'], max_dbz=config['max_dbz'],\n",
    "                                   expected_shape=config['expected_shape'], variable_name=config['dbz_variable_name'])\n",
    "        val_dataset_len = len(val_dataset)\n",
    "        if val_dataset_len > 0:\n",
    "             val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "        else: logging.info(\"Dataset de validación vacío.\")\n",
    "    else: logging.info(\"No subdirectorios para validación.\")\n",
    "\n",
    "    # Necesitamos datos para generar predicciones, al menos de validación o entrenamiento\n",
    "    if not val_loader and not train_loader:\n",
    "        logging.error(\"No hay datos de validación ni de entrenamiento para generar predicciones.\")\n",
    "        return\n",
    "\n",
    "    model = ConvLSTM3D_Enhanced(\n",
    "        input_dim=config['model_input_dim'], hidden_dims=config['model_hidden_dims'],\n",
    "        kernel_sizes=config['model_kernel_sizes'], num_layers=config['model_num_layers'],\n",
    "        pred_steps=config['pred_steps_model'], use_layer_norm=config['model_use_layer_norm'],\n",
    "        use_residual=config['model_use_residual'],\n",
    "        img_height=config['expected_shape'][1], img_width=config['expected_shape'][2]\n",
    "    )\n",
    "    model.float() # Asegurar que el modelo se inicialice en float32\n",
    "\n",
    "    logging.info(f\"Arquitectura del modelo:\\n{model}\")\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logging.info(f\"Número total de parámetros entrenables: {total_params:,}\")\n",
    "\n",
    "    device_for_execution = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_path = os.path.join(config['model_save_dir'], \"best_convlstm_model.pth\")\n",
    "    if os.path.exists(model_path):\n",
    "        logging.info(f\"Cargando modelo pre-entrenado desde: {model_path}\")\n",
    "        # Cargar a CPU, luego asegurar .float(), luego mover a device\n",
    "        checkpoint_data = torch.load(model_path, map_location='cpu', weights_only=True) #weights_only=True por seguridad\n",
    "        model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "        model.float() # Asegurar float32 después de cargar\n",
    "        logging.info(f\"Modelo cargado. Dtype parámetros: {next(model.parameters()).dtype}\")\n",
    "        trained_model = model\n",
    "    else:\n",
    "        logging.info(\"No se encontró modelo pre-entrenado. Entrenando desde cero...\")\n",
    "        if not train_loader:\n",
    "            logging.error(\"No hay datos de entrenamiento y no se encontró modelo pre-entrenado. Saliendo.\")\n",
    "            return\n",
    "        trained_model, history = train_model(model, train_loader, val_loader, config) # train_model se encarga de .to(device)\n",
    "\n",
    "    trained_model.to(device_for_execution) # Mover el modelo final al dispositivo\n",
    "    trained_model.float() # Re-asegurar float32 después de mover (por si acaso)\n",
    "    logging.info(f\"Modelo listo para predicción. Dtype: {next(trained_model.parameters()).dtype}, Dispositivo: {next(trained_model.parameters()).device}\")\n",
    "\n",
    "    # Priorizar val_loader para predicciones, si no, usar train_loader\n",
    "    prediction_loader = val_loader if val_loader and val_dataset_len > 0 else train_loader\n",
    "    num_prediction_samples = min(5, val_dataset_len if val_loader and val_dataset_len > 0 else (len(train_loader.dataset) if train_loader else 0))\n",
    "\n",
    "    if prediction_loader and num_prediction_samples > 0:\n",
    "        logging.info(\"Generando predicciones de ejemplo...\")\n",
    "        generate_prediction_netcdf(trained_model, prediction_loader, config,\n",
    "                                   device=device_for_execution,\n",
    "                                   num_samples=num_prediction_samples)\n",
    "    else:\n",
    "        logging.warning(\"No hay datos disponibles en val_loader o train_loader para generar predicciones de ejemplo.\")\n",
    "\n",
    "    logging.info(\"Proceso completado.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eec8c64-b9ab-4b1a-a742-d0bca5569527",
   "metadata": {
    "id": "7eec8c64-b9ab-4b1a-a742-d0bca5569527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyproj\n",
      "  Downloading pyproj-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from pyproj) (2024.8.30)\n",
      "Downloading pyproj-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyproj\n",
      "Successfully installed pyproj-3.7.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae001fd-5f38-499d-acdc-b5851649dbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
