{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6399b35-6488-4a16-92d6-d7d9acfe944d",
   "metadata": {
    "id": "e6399b35-6488-4a16-92d6-d7d9acfe944d",
    "outputId": "689dccdb-fb13-4a4c-8968-ae9f4c6fbf56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH_CUDA_ALLOC_CONF set to: expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "print(f\"PYTORCH_CUDA_ALLOC_CONF set to: {os.environ.get('PYTORCH_CUDA_ALLOC_CONF')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a95315-21c1-41de-a409-09f3319d9b68",
   "metadata": {
    "id": "98a95315-21c1-41de-a409-09f3319d9b68",
    "outputId": "3902861a-46cd-4c01-af9c-d236f1cd9edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netCDF4 in /opt/conda/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.10.3)\n",
      "Requirement already satisfied: cftime in /opt/conda/lib/python3.11/site-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from netCDF4) (2024.8.30)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from netCDF4) (2.1.2)\n",
      "Requirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (24.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (2.5.1+cu121)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (0.14.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (72.1.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install netCDF4 torchmetrics matplotlib\n",
    "!pip install pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "BrhYI9SSVo_t",
   "metadata": {
    "id": "BrhYI9SSVo_t",
    "outputId": "0a7831e7-cd07-446e-badb-1dc03a590380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f803dd-b188-46c0-96a9-496c8ae389ea",
   "metadata": {
    "id": "02f803dd-b188-46c0-96a9-496c8ae389ea",
    "outputId": "f621b64f-534d-4e6c-c68b-e27c2c70e147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1mTl6jomSku9xiDvtw8o_qia4iRFUGjYY\n",
      "From (redirected): https://drive.google.com/uc?id=1mTl6jomSku9xiDvtw8o_qia4iRFUGjYY&confirm=t&uuid=4109d594-fa90-44a2-9d73-1e9d128bfce1\n",
      "To: /home/sample.tar.gz\n",
      "100%|██████████████████████████████████████| 3.00G/3.00G [00:37<00:00, 79.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "!cd /home\n",
    "!gdown \"https://drive.google.com/uc?id=1mTl6jomSku9xiDvtw8o_qia4iRFUGjYY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46053838-a6f8-4835-9572-b7a409d2f94a",
   "metadata": {
    "id": "46053838-a6f8-4835-9572-b7a409d2f94a",
    "outputId": "6a68f00c-b475-407f-90a5-ebbb4324abe1"
   },
   "outputs": [],
   "source": [
    "!tar -xzvf /home/sample.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec770cc-2e7d-4ca2-b908-7ee098a5b596",
   "metadata": {
    "id": "9ec770cc-2e7d-4ca2-b908-7ee098a5b596",
    "outputId": "392d73ea-0d1f-4f22-b60b-3d378251092b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version PyTorch built with: 12.1\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA H100 NVL\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version PyTorch built with: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb893018-e461-4170-bd08-9cefba404415",
   "metadata": {
    "id": "eb893018-e461-4170-bd08-9cefba404415",
    "outputId": "659baab6-c872-42d7-ee99-a44ea00fd074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio '/home/sample' contiene 370 subcarpetas (directorios).\n",
      "\n",
      "Algunas de las subcarpetas encontradas:\n",
      "- 2015122110\n",
      "- 201511021\n",
      "- 201612177\n",
      "- 201702244\n",
      "- 201612171\n",
      "- 201603033\n",
      "- 201703276\n",
      "- 201812263\n",
      "- 2016020417\n",
      "- 201601232\n",
      "... y 360 más.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# El path base que quieres inspeccionar\n",
    "base_path = \"/home/sample\"\n",
    "\n",
    "# Verificar si el path base existe\n",
    "if not os.path.exists(base_path):\n",
    "    print(f\"Error: El directorio base '{base_path}' no existe.\")\n",
    "else:\n",
    "    # Listar todos los contenidos del directorio base\n",
    "    try:\n",
    "        all_contents = os.listdir(base_path)\n",
    "\n",
    "        # Filtrar para quedarnos solo con los directorios\n",
    "        subdirectories = [d for d in all_contents if os.path.isdir(os.path.join(base_path, d))]\n",
    "\n",
    "        # Contar la cantidad de subdirectorios\n",
    "        num_subdirectories = len(subdirectories)\n",
    "\n",
    "        print(f\"El directorio '{base_path}' contiene {num_subdirectories} subcarpetas (directorios).\")\n",
    "\n",
    "        # Opcional: Imprimir los primeros N nombres de subcarpetas para verificar\n",
    "        if num_subdirectories > 0:\n",
    "            print(\"\\nAlgunas de las subcarpetas encontradas:\")\n",
    "            for i, subdir_name in enumerate(subdirectories):\n",
    "                if i < 10: # Imprime las primeras 10 (o menos si hay menos)\n",
    "                    print(f\"- {subdir_name}\")\n",
    "                else:\n",
    "                    break\n",
    "            if num_subdirectories > 10:\n",
    "                print(f\"... y {num_subdirectories - 10} más.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al intentar listar los contenidos de '{base_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c5fd3-2adb-477c-a88c-43534f9e647a",
   "metadata": {
    "id": "da8c5fd3-2adb-477c-a88c-43534f9e647a",
    "outputId": "13641949-af46-44a2-a46a-c47df8cc0195"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 21:25:23,094 - INFO - Semillas configuradas con valor: 42\n",
      "2025-06-01 21:25:23,095 - INFO - Usando muestra aleatoria de 10 secuencias.\n",
      "2025-06-01 21:25:23,096 - INFO - Total secuencias a usar: 10.\n",
      "2025-06-01 21:25:23,096 - INFO - Entrenamiento: 8 sec. Validación: 2 sec.\n",
      "2025-06-01 21:25:23,134 - INFO - Archivo /home/sample/202002023/202200.nc: Min=-30.00, Max=63.99\n",
      "2025-06-01 21:25:23,170 - INFO - Archivo /home/sample/201911116/200242.nc: Min=-30.00, Max=60.41\n",
      "2025-06-01 21:25:23,205 - INFO - Archivo /home/sample/2020020314/210448.nc: Min=-30.00, Max=62.44\n",
      "2025-06-01 21:25:23,238 - INFO - Archivo /home/sample/201812274/014714.nc: Min=-30.00, Max=62.59\n",
      "2025-06-01 21:25:23,273 - INFO - Archivo /home/sample/2017032915/140403.nc: Min=-30.00, Max=63.42\n",
      "2025-06-01 21:25:23,306 - INFO - Archivo /home/sample/2020020315/214849.nc: Min=-30.00, Max=62.52\n",
      "2025-06-01 21:25:23,341 - INFO - Archivo /home/sample/201911247/162417.nc: Min=-30.00, Max=60.54\n",
      "2025-06-01 21:25:23,375 - INFO - Archivo /home/sample/2017032713/103508.nc: Min=-30.00, Max=65.46\n",
      "2025-06-01 21:25:23,378 - INFO - RadarDataset inicializado con 8 secuencias válidas.\n",
      "2025-06-01 21:25:23,416 - INFO - Archivo /home/sample/201511169/210649.nc: Min=-30.00, Max=58.14\n",
      "2025-06-01 21:25:23,452 - INFO - Archivo /home/sample/201911106/214525.nc: Min=-30.00, Max=61.14\n",
      "2025-06-01 21:25:23,455 - INFO - RadarDataset inicializado con 2 secuencias válidas.\n",
      "2025-06-01 21:25:23,462 - INFO - Modelo ConvLSTM3D_Enhanced creado: 2 capas, Hidden dims: [32, 32], LayerNorm: True, PredSteps: 1\n",
      "2025-06-01 21:25:23,462 - INFO - Arquitectura del modelo:\n",
      "ConvLSTM3D_Enhanced(\n",
      "  (layers): ModuleList(\n",
      "    (0): ConvLSTM2DLayer(\n",
      "      (cell): ConvLSTMCell(\n",
      "        (conv): Conv2d(33, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): ConvLSTM2DLayer(\n",
      "      (cell): ConvLSTMCell(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer_norms): ModuleList(\n",
      "    (0-1): 2 x LayerNorm((32, 500, 500), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (output_conv): Conv3d(32, 1, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "2025-06-01 21:25:23,462 - INFO - Número total de parámetros entrenables: 32,112,289\n",
      "2025-06-01 21:25:23,463 - INFO - No se encontró modelo pre-entrenado. Entrenando desde cero...\n",
      "2025-06-01 21:25:23,463 - INFO - Usando dispositivo: cuda\n",
      "2025-06-01 21:25:23,476 - INFO - Iniciando entrenamiento: 2 épocas, LR: 0.001, Batch (efectivo): 1\n",
      "2025-06-01 21:25:23,537 - INFO - Inicio Época 1 - Memoria GPU Asignada: 0.24 GB, Reservada: 0.26 GB\n",
      "2025-06-01 21:25:23,648 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:23,649 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0036\n",
      "2025-06-01 21:25:23,730 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0072\n",
      "2025-06-01 21:25:23,735 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0035\n",
      "2025-06-01 21:25:23,811 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0068\n",
      "2025-06-01 21:25:23,818 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0032\n",
      "2025-06-01 21:25:23,897 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0075\n",
      "2025-06-01 21:25:23,902 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0031\n",
      "2025-06-01 21:25:23,978 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0072\n",
      "2025-06-01 21:25:23,983 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0027\n",
      "2025-06-01 21:25:24,058 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0080\n",
      "2025-06-01 21:25:24,064 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0027\n",
      "2025-06-01 21:25:24,139 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:24,149 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0026\n",
      "2025-06-01 21:25:24,188 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:24,199 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0026\n",
      "2025-06-01 21:25:24,468 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0052\n",
      "2025-06-01 21:25:24,488 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0077\n",
      "2025-06-01 21:25:24,550 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:24,569 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:24,626 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0055\n",
      "2025-06-01 21:25:24,646 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:24,703 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0057\n",
      "2025-06-01 21:25:24,723 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:24,780 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0055\n",
      "2025-06-01 21:25:24,799 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0087\n",
      "2025-06-01 21:25:24,856 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0059\n",
      "2025-06-01 21:25:24,875 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0086\n",
      "2025-06-01 21:25:24,932 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0061\n",
      "2025-06-01 21:25:24,964 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:24,979 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0061\n",
      "2025-06-01 21:25:25,010 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:25,161 - INFO -   Predicciones (normalizadas): Min=0.0000, Max=1.0000, Mean=0.5034\n",
      "2025-06-01 21:25:25,162 - INFO -   Objetivos y (normalizados): Min=0.0000, Max=1.0000, Mean=0.0026\n",
      "2025-06-01 21:25:25,294 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0019\n",
      "2025-06-01 21:25:25,368 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0020\n",
      "2025-06-01 21:25:25,440 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:25,509 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0018\n",
      "2025-06-01 21:25:25,580 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:25,650 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:25,720 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:25,752 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:26,003 - INFO - Época 1/2 [1/8] - Pérdida (batch): 0.253819\n",
      "2025-06-01 21:25:26,081 - INFO - dbz_final: Min=0.0000, Max=0.9914, Mean=0.0038\n",
      "2025-06-01 21:25:26,157 - INFO - dbz_final: Min=0.0000, Max=0.9899, Mean=0.0040\n",
      "2025-06-01 21:25:26,227 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0041\n",
      "2025-06-01 21:25:26,295 - INFO - dbz_final: Min=0.0000, Max=0.9915, Mean=0.0042\n",
      "2025-06-01 21:25:26,363 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0040\n",
      "2025-06-01 21:25:26,429 - INFO - dbz_final: Min=0.0000, Max=0.9949, Mean=0.0044\n",
      "2025-06-01 21:25:26,496 - INFO - dbz_final: Min=0.0000, Max=0.9990, Mean=0.0047\n",
      "2025-06-01 21:25:26,527 - INFO - Output tensor: Min=0.0000, Max=0.9990, Mean=0.0047\n",
      "2025-06-01 21:25:27,571 - INFO - Época 1/2 [2/8] - Pérdida (batch): 0.255369\n",
      "2025-06-01 21:25:27,651 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0150\n",
      "2025-06-01 21:25:27,722 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0144\n",
      "2025-06-01 21:25:27,790 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0143\n",
      "2025-06-01 21:25:27,858 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0133\n",
      "2025-06-01 21:25:27,926 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0124\n",
      "2025-06-01 21:25:27,994 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0136\n",
      "2025-06-01 21:25:28,060 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0126\n",
      "2025-06-01 21:25:28,091 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0126\n",
      "2025-06-01 21:25:29,139 - INFO - Época 1/2 [3/8] - Pérdida (batch): 0.255184\n",
      "2025-06-01 21:25:29,218 - INFO - dbz_final: Min=0.0000, Max=0.9945, Mean=0.0034\n",
      "2025-06-01 21:25:29,290 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0039\n",
      "2025-06-01 21:25:29,358 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:29,426 - INFO - dbz_final: Min=0.0000, Max=0.9748, Mean=0.0034\n",
      "2025-06-01 21:25:29,492 - INFO - dbz_final: Min=0.0000, Max=0.9905, Mean=0.0031\n",
      "2025-06-01 21:25:29,560 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0033\n",
      "2025-06-01 21:25:29,628 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:29,659 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:30,707 - INFO - Época 1/2 [4/8] - Pérdida (batch): 0.254672\n",
      "2025-06-01 21:25:32,273 - INFO - Época 1/2 [5/8] - Pérdida (batch): 0.252034\n",
      "2025-06-01 21:25:33,839 - INFO - Época 1/2 [6/8] - Pérdida (batch): 0.253066\n",
      "2025-06-01 21:25:35,407 - INFO - Época 1/2 [7/8] - Pérdida (batch): 0.257150\n",
      "2025-06-01 21:25:36,975 - INFO - Época 1/2 [8/8] - Pérdida (batch): 0.005556\n",
      "2025-06-01 21:25:37,118 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:37,118 - INFO - dbz_final: Min=0.0000, Max=0.9991, Mean=0.0055\n",
      "2025-06-01 21:25:37,203 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:37,203 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0054\n",
      "2025-06-01 21:25:37,286 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:37,289 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0086\n",
      "2025-06-01 21:25:37,369 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0052\n",
      "2025-06-01 21:25:37,370 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:37,452 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:37,454 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0094\n",
      "2025-06-01 21:25:37,538 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0091\n",
      "2025-06-01 21:25:37,538 - INFO - dbz_final: Min=0.0000, Max=0.9932, Mean=0.0053\n",
      "2025-06-01 21:25:37,623 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "2025-06-01 21:25:37,623 - INFO - dbz_final: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:37,675 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "2025-06-01 21:25:37,675 - INFO - Output tensor: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:39,386 - INFO - Época 1 completada en 15.85s. Pérdida (train): 0.223356, Pérdida (val): 0.004874\n",
      "2025-06-01 21:25:39,544 - INFO - Mejor modelo guardado (Pérdida Val: 0.004874)\n",
      "2025-06-01 21:25:39,692 - INFO - Checkpoint guardado en la época 1\n",
      "2025-06-01 21:25:39,753 - INFO - Inicio Época 2 - Memoria GPU Asignada: 0.76 GB, Reservada: 0.84 GB\n",
      "2025-06-01 21:25:39,877 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0019\n",
      "2025-06-01 21:25:39,879 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0150\n",
      "2025-06-01 21:25:39,964 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0020\n",
      "2025-06-01 21:25:39,967 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0144\n",
      "2025-06-01 21:25:40,046 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:40,052 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0143\n",
      "2025-06-01 21:25:40,127 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0018\n",
      "2025-06-01 21:25:40,134 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0133\n",
      "2025-06-01 21:25:40,210 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:40,214 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0124\n",
      "2025-06-01 21:25:40,292 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:40,297 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0136\n",
      "2025-06-01 21:25:40,371 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:40,380 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0126\n",
      "2025-06-01 21:25:40,420 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0017\n",
      "2025-06-01 21:25:40,428 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0126\n",
      "2025-06-01 21:25:40,688 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:40,713 - INFO - dbz_final: Min=0.0000, Max=0.9914, Mean=0.0038\n",
      "2025-06-01 21:25:40,762 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0072\n",
      "2025-06-01 21:25:40,788 - INFO - dbz_final: Min=0.0000, Max=0.9899, Mean=0.0040\n",
      "2025-06-01 21:25:40,837 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0068\n",
      "2025-06-01 21:25:40,866 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0041\n",
      "2025-06-01 21:25:40,913 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0075\n",
      "2025-06-01 21:25:40,942 - INFO - dbz_final: Min=0.0000, Max=0.9915, Mean=0.0042\n",
      "2025-06-01 21:25:40,987 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0072\n",
      "2025-06-01 21:25:41,017 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0040\n",
      "2025-06-01 21:25:41,060 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0080\n",
      "2025-06-01 21:25:41,089 - INFO - dbz_final: Min=0.0000, Max=0.9949, Mean=0.0044\n",
      "2025-06-01 21:25:41,134 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:41,179 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:41,179 - INFO - dbz_final: Min=0.0000, Max=0.9990, Mean=0.0047\n",
      "2025-06-01 21:25:41,229 - INFO - Output tensor: Min=0.0000, Max=0.9990, Mean=0.0047\n",
      "2025-06-01 21:25:41,482 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0052\n",
      "2025-06-01 21:25:41,554 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:41,624 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0055\n",
      "2025-06-01 21:25:41,694 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0057\n",
      "2025-06-01 21:25:41,764 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0055\n",
      "2025-06-01 21:25:41,833 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0059\n",
      "2025-06-01 21:25:41,904 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0061\n",
      "2025-06-01 21:25:41,935 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0061\n",
      "2025-06-01 21:25:42,235 - INFO - Época 2/2 [1/8] - Pérdida (batch): 0.008567\n",
      "2025-06-01 21:25:42,308 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0036\n",
      "2025-06-01 21:25:42,381 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0035\n",
      "2025-06-01 21:25:42,451 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0032\n",
      "2025-06-01 21:25:42,522 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0031\n",
      "2025-06-01 21:25:42,592 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0027\n",
      "2025-06-01 21:25:42,662 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0027\n",
      "2025-06-01 21:25:42,732 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0026\n",
      "2025-06-01 21:25:42,763 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0026\n",
      "2025-06-01 21:25:43,802 - INFO - Época 2/2 [2/8] - Pérdida (batch): 0.000763\n",
      "2025-06-01 21:25:43,874 - INFO - dbz_final: Min=0.0000, Max=0.9945, Mean=0.0034\n",
      "2025-06-01 21:25:43,947 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0039\n",
      "2025-06-01 21:25:44,017 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:44,088 - INFO - dbz_final: Min=0.0000, Max=0.9748, Mean=0.0034\n",
      "2025-06-01 21:25:44,157 - INFO - dbz_final: Min=0.0000, Max=0.9905, Mean=0.0031\n",
      "2025-06-01 21:25:44,226 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0033\n",
      "2025-06-01 21:25:44,296 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:44,327 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0037\n",
      "2025-06-01 21:25:45,370 - INFO - Época 2/2 [3/8] - Pérdida (batch): 0.000991\n",
      "2025-06-01 21:25:45,441 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0077\n",
      "2025-06-01 21:25:45,513 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0083\n",
      "2025-06-01 21:25:45,585 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:45,655 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0082\n",
      "2025-06-01 21:25:45,726 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0087\n",
      "2025-06-01 21:25:45,795 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0086\n",
      "2025-06-01 21:25:45,864 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:45,895 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:46,940 - INFO - Época 2/2 [4/8] - Pérdida (batch): 0.002652\n",
      "2025-06-01 21:25:48,509 - INFO - Época 2/2 [5/8] - Pérdida (batch): 0.001149\n",
      "2025-06-01 21:25:50,078 - INFO - Época 2/2 [6/8] - Pérdida (batch): 0.000655\n",
      "2025-06-01 21:25:51,648 - INFO - Época 2/2 [7/8] - Pérdida (batch): 0.000887\n",
      "2025-06-01 21:25:53,216 - INFO - Época 2/2 [8/8] - Pérdida (batch): 0.001900\n",
      "2025-06-01 21:25:53,377 - INFO - dbz_final: Min=0.0000, Max=0.9991, Mean=0.0055\n",
      "2025-06-01 21:25:53,384 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:53,460 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0054\n",
      "2025-06-01 21:25:53,468 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:53,537 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:53,551 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0086\n",
      "2025-06-01 21:25:53,611 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0052\n",
      "2025-06-01 21:25:53,629 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:53,683 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:53,706 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0094\n",
      "2025-06-01 21:25:53,759 - INFO - dbz_final: Min=0.0000, Max=0.9932, Mean=0.0053\n",
      "2025-06-01 21:25:53,784 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0091\n",
      "2025-06-01 21:25:53,834 - INFO - dbz_final: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:53,875 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "2025-06-01 21:25:53,878 - INFO - Output tensor: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:53,927 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "2025-06-01 21:25:55,587 - INFO - Época 2 completada en 15.84s. Pérdida (train): 0.002196, Pérdida (val): 0.001731\n",
      "2025-06-01 21:25:55,801 - INFO - Mejor modelo guardado (Pérdida Val: 0.001731)\n",
      "2025-06-01 21:25:55,951 - INFO - Checkpoint guardado en la época 2\n",
      "2025-06-01 21:25:55,951 - INFO - Entrenamiento finalizado.\n",
      "2025-06-01 21:25:55,986 - INFO - Curvas de pérdida guardadas en /home/model_output_final_v_ckpt/loss_curves.png\n",
      "2025-06-01 21:25:55,988 - INFO - Modelo listo para predicción. Dtype: torch.float32, Dispositivo: cuda:0\n",
      "2025-06-01 21:25:55,988 - INFO - Generando predicciones de ejemplo...\n",
      "2025-06-01 21:25:56,134 - INFO - dbz_final: Min=0.0000, Max=0.9991, Mean=0.0055\n",
      "2025-06-01 21:25:56,138 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:56,216 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0054\n",
      "2025-06-01 21:25:56,223 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0085\n",
      "2025-06-01 21:25:56,293 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:56,305 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0086\n",
      "2025-06-01 21:25:56,368 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0052\n",
      "2025-06-01 21:25:56,381 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0088\n",
      "2025-06-01 21:25:56,441 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0056\n",
      "2025-06-01 21:25:56,458 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0094\n",
      "2025-06-01 21:25:56,518 - INFO - dbz_final: Min=0.0000, Max=0.9932, Mean=0.0053\n",
      "2025-06-01 21:25:56,541 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0091\n",
      "2025-06-01 21:25:56,594 - INFO - dbz_final: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:56,636 - INFO - dbz_final: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "2025-06-01 21:25:56,644 - INFO - Output tensor: Min=0.0000, Max=0.9736, Mean=0.0056\n",
      "2025-06-01 21:25:56,686 - INFO - Output tensor: Min=0.0000, Max=1.0000, Mean=0.0097\n",
      "/opt/conda/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "2025-06-01 21:25:57,649 - INFO - Predicción Física Desnormalizada (muestra 0): Min=-39.91, Max=41.31, Mean=-inf\n",
      "/tmp/ipykernel_92/2418571061.py:601: RuntimeWarning: invalid value encountered in cast\n",
      "  np.clip(((pred_data_for_packing - output_add_offset) / output_scale_factor), -127, 127).round().astype(np.int8))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF predicción t+3min guardado: /home/predictions_final_v_ckpt/pred_DBZ_20250601_211056_sample0.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 21:25:58,473 - INFO - Predicción Física Desnormalizada (muestra 1): Min=-39.91, Max=41.38, Mean=-inf\n",
      "2025-06-01 21:25:58,549 - INFO - Proceso completado.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF predicción t+3min guardado: /home/predictions_final_v_ckpt/pred_DBZ_20250601_211057_sample1.nc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta # Asegúrate de importar timedelta\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from netCDF4 import Dataset as NCDataset # Renombrar para evitar conflicto con la clase Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics # Para métricas adicionales como SSIM\n",
    "from torch.utils.checkpoint import checkpoint # <--- IMPORTANTE PARA GRADIENT CHECKPOINTING\n",
    "import torch.amp # <--- IMPORTANTE PARA APIS MODERNAS DE AMP\n",
    "\n",
    "import torch.optim as optim\n",
    "import pyproj #\n",
    "\n",
    "\n",
    "# Configuración del Logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuración para reproducibilidad y rendimiento\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # Necesario si usas múltiples GPUs\n",
    "        # La siguiente configuración es un balance entre reproducibilidad y rendimiento\n",
    "        torch.backends.cudnn.deterministic = False # Para reproducibilidad estricta, sería True\n",
    "        torch.backends.cudnn.benchmark = True    # Para reproducibilidad estricta, sería False. Acelera si los tamaños de entrada no cambian.\n",
    "    logging.info(f\"Semillas configuradas con valor: {seed}\")\n",
    "\n",
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, data_dir, subdirs_list, seq_len=6, pred_len=1,\n",
    "                 min_dbz_norm=-30.0, max_dbz_norm=70.0, # Cambiado para usar estos nombres y valores por defecto consistentes\n",
    "                 expected_shape=(18, 500, 500), variable_name='DBZ'):\n",
    "        self.data_dir = data_dir\n",
    "        self.subdirs_list = subdirs_list\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        # Usar los parámetros pasados para normalización\n",
    "        self.min_dbz_norm = min_dbz_norm\n",
    "        self.max_dbz_norm = max_dbz_norm\n",
    "        self.expected_z, self.expected_h, self.expected_w = expected_shape\n",
    "        self.variable_name = variable_name\n",
    "        self.valid_sequences = self._validate_subdirs()\n",
    "        if not self.valid_sequences:\n",
    "            logging.error(\"No se encontraron secuencias válidas. Verifica los datos y la estructura de carpetas.\")\n",
    "            raise ValueError(\"No se encontraron secuencias válidas.\")\n",
    "        logging.info(f\"RadarDataset inicializado con {len(self.valid_sequences)} secuencias válidas.\")\n",
    "\n",
    "    def _validate_subdirs(self):\n",
    "        valid_sequences = []\n",
    "        for subdir_name in self.subdirs_list:\n",
    "            subdir_path = os.path.join(self.data_dir, subdir_name)\n",
    "            if not os.path.isdir(subdir_path):\n",
    "                logging.warning(f\"Subdirectorio {subdir_name} no encontrado...\")\n",
    "                continue\n",
    "            if \".ipynb_checkpoints\" in subdir_name: #\n",
    "                logging.debug(f\"Omitiendo directorio de checkpoints: {subdir_name}\") #\n",
    "                continue\n",
    "            files = sorted(glob.glob(os.path.join(subdir_path, \"*.nc\")))\n",
    "            if len(files) >= self.seq_len + self.pred_len:\n",
    "                output_files = files[self.seq_len:self.seq_len + self.pred_len] #\n",
    "                valid_output = True\n",
    "                for f_path in output_files: # Cambiado 'f' por 'f_path' para evitar confusión con la f-string\n",
    "                    try:\n",
    "                        with NCDataset(f_path, 'r') as nc_file:\n",
    "                            if self.variable_name not in nc_file.variables: #\n",
    "                                valid_output = False\n",
    "                                logging.warning(f\"Variable {self.variable_name} no encontrada en archivo de salida {f_path}\") #\n",
    "                                break\n",
    "                            dbz_var = nc_file.variables[self.variable_name]\n",
    "                            data_raw = dbz_var[0, ...] if dbz_var.ndim == 4 else dbz_var[...] #\n",
    "                            \n",
    "                            # Aplicar scale/offset para obtener valores físicos para el log\n",
    "                            dbz_physical_for_log = data_raw # Asumir que ya es físico si no hay scale/offset\n",
    "                            if hasattr(dbz_var, 'scale_factor') and hasattr(dbz_var, 'add_offset'):\n",
    "                                scale = dbz_var.scale_factor\n",
    "                                offset = dbz_var.add_offset\n",
    "                                dbz_physical_for_log = data_raw.astype(np.float32) * scale + offset\n",
    "                                if hasattr(dbz_var, '_FillValue'):\n",
    "                                    fill_val_packed = getattr(dbz_var, '_FillValue')\n",
    "                                    # Convertir fill_val_packed al dtype de data_raw para comparación segura\n",
    "                                    is_fill = (data_raw == np.array(fill_val_packed, dtype=data_raw.dtype))\n",
    "                                    dbz_physical_for_log[is_fill] = np.nan\n",
    "\n",
    "                            # Loguear min/max de valores físicos (ignorando NaN)\n",
    "                            logging.info(f\"Archivo validación {os.path.basename(f_path)}: Físico Min={np.nanmin(dbz_physical_for_log):.2f}, Max={np.nanmax(dbz_physical_for_log):.2f}\") #\n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Error leyendo archivo de salida {f_path}: {e}\") #\n",
    "                        valid_output = False\n",
    "                        break\n",
    "                if valid_output:\n",
    "                    valid_sequences.append((files, subdir_name))\n",
    "            else:\n",
    "                logging.warning(f\"Subdirectorio {subdir_name} tiene {len(files)} archivos, se necesitan {self.seq_len + self.pred_len}...\") #\n",
    "        return valid_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_files, subdir_name = self.valid_sequences[idx]\n",
    "        input_data_list = []\n",
    "        output_data_list = []\n",
    "\n",
    "        all_files_for_sequence = sequence_files[:self.seq_len + self.pred_len]\n",
    "\n",
    "        for i, file_path in enumerate(all_files_for_sequence):\n",
    "            try:\n",
    "                with NCDataset(file_path, 'r') as nc_file:\n",
    "                    if self.variable_name not in nc_file.variables:\n",
    "                        logging.warning(f\"Variable '{self.variable_name}' no encontrada en {file_path}. Intentando siguiente muestra.\")\n",
    "                        return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "                    dbz_var = nc_file.variables[self.variable_name]\n",
    "\n",
    "                    # 1. Cargar datos raw (byte/short) SIN convertirlos a float todavía\n",
    "                    if dbz_var.ndim == 4 and dbz_var.shape[0] == 1:\n",
    "                        data_raw_original_type = dbz_var[0, ...] \n",
    "                    elif dbz_var.ndim == 3:\n",
    "                        data_raw_original_type = dbz_var[...]\n",
    "                    else:\n",
    "                        # ... (manejo de error) ...\n",
    "                        return self.__getitem__((idx + 1) % len(self))\n",
    "                    \n",
    "                    # 2. Crear la máscara de fill_value A PARTIR DE LOS DATOS RAW ORIGINALES\n",
    "                    is_fill_location = None\n",
    "                    if hasattr(dbz_var, '_FillValue'):\n",
    "                        fill_value_packed = dbz_var._FillValue \n",
    "                        logging.debug(f\"FILE: {os.path.basename(file_path)} - data_raw_original_type.dtype: {data_raw_original_type.dtype}, fill_value_packed: {fill_value_packed} (type: {type(fill_value_packed)})\")\n",
    "                        # Comparación directa con el tipo original de data_raw_original_type\n",
    "                        is_fill_location = (data_raw_original_type == np.array(fill_value_packed, dtype=data_raw_original_type.dtype))\n",
    "                        if is_fill_location is not None:\n",
    "                            logging.info(f\"FILE: {os.path.basename(file_path)} - Num True en is_fill_location: {np.sum(is_fill_location)}\")\n",
    "                        else: # Esto no debería ocurrir si hasattr es True\n",
    "                            logging.info(f\"FILE: {os.path.basename(file_path)} - is_fill_location es None.\")\n",
    "                    \n",
    "                    # 3. Convertir data_raw a float32 para cálculos físicos\n",
    "                    dbz_physical = data_raw_original_type.astype(np.float32)\n",
    "\n",
    "                    # 4. Aplicar scale/offset para obtener valores físicos\n",
    "                    if hasattr(dbz_var, 'scale_factor') and hasattr(dbz_var, 'add_offset'):\n",
    "                        scale = dbz_var.scale_factor\n",
    "                        offset = dbz_var.add_offset\n",
    "                        dbz_physical = dbz_physical * scale + offset # Aplicado a la copia float32\n",
    "\n",
    "                    # 5. Aplicar la máscara de NaN AHORA a los datos físicos (dbz_physical)\n",
    "                    if is_fill_location is not None:\n",
    "                        dbz_physical[is_fill_location] = np.nan # Los NaNs se introducen aquí\n",
    "\n",
    "                    # dbz_physical ahora tiene valores físicos, con NaN donde había _FillValue\n",
    "\n",
    "                    # Logging para depuración (NIVEL DEBUG)\n",
    "                    logging.debug(f\"FILE: {os.path.basename(file_path)} - dbz_physical (con NaNs): \"\n",
    "                                 f\"NanMin={np.nanmin(dbz_physical):.2f}, NanMax={np.nanmax(dbz_physical):.2f}, \"\n",
    "                                 f\"NumNaNs={np.isnan(dbz_physical).sum()}\")\n",
    "                    \n",
    "                    # 6. Clipping y Normalización, preservando NaN\n",
    "                    dbz_clipped = np.clip(dbz_physical, self.min_dbz_norm, self.max_dbz_norm) # np.clip propaga NaNs\n",
    "                    logging.debug(f\"FILE: {os.path.basename(file_path)} - dbz_clipped (con NaNs): \"\n",
    "                                 f\"NanMin={np.nanmin(dbz_clipped):.2f}, NanMax={np.nanmax(dbz_clipped):.2f}, \"\n",
    "                                 f\"NumNaNs={np.isnan(dbz_clipped).sum()}\")\n",
    "                    \n",
    "\n",
    "                    # Denominador para la normalización\n",
    "                    range_dbz = self.max_dbz_norm - self.min_dbz_norm\n",
    "                    if range_dbz == 0: # Evitar división por cero\n",
    "                        logging.warning(f\"Rango de normalización es cero (min_dbz_norm == max_dbz_norm) en {file_path}. Asignando 0 a los valores no NaN.\")\n",
    "                        dbz_normalized = np.where(np.isnan(dbz_clipped), np.nan, 0.0)\n",
    "                    else:\n",
    "                        dbz_normalized = (dbz_clipped - self.min_dbz_norm) / range_dbz\n",
    "\n",
    "\n",
    "                    logging.debug(f\"FILE: {os.path.basename(file_path)} - dbz_normalized (esperado [0,1] y NaN): \"\n",
    "                                 f\"NanMin={np.nanmin(dbz_normalized):.4f}, NanMax={np.nanmax(dbz_normalized):.4f}, \"\n",
    "                                 f\"NanMean={np.nanmean(dbz_normalized):.4f}, NumNaNs={np.isnan(dbz_normalized).sum()}\")\n",
    "                    \n",
    "                    if dbz_normalized.shape != (self.expected_z, self.expected_h, self.expected_w):\n",
    "                        logging.warning(f\"Forma inesperada {dbz_normalized.shape} en {file_path}. Intentando siguiente muestra.\")\n",
    "                        return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "                    dbz_final = dbz_normalized[..., np.newaxis]\n",
    "\n",
    "                    if i < self.seq_len:\n",
    "                        input_data_list.append(dbz_final)\n",
    "                    else:\n",
    "                        output_data_list.append(dbz_final)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error procesando archivo {file_path} en __getitem__: {e}\") #\n",
    "                return self.__getitem__((idx + 1) % len(self)) #\n",
    "\n",
    "        if len(input_data_list) != self.seq_len or len(output_data_list) != self.pred_len: #\n",
    "            logging.warning(f\"No se pudieron cargar suficientes frames para la secuencia {subdir_name}. Intentando siguiente muestra.\") #\n",
    "            return self.__getitem__((idx + 1) % len(self)) #\n",
    "\n",
    "        input_tensor = np.stack(input_data_list, axis=1)\n",
    "        output_tensor = np.stack(output_data_list, axis=1)\n",
    "        logging.info(f\"RadarDataset: output_tensor (ANTES de from_numpy): \"\n",
    "                    f\"NanMin={np.nanmin(output_tensor):.4f}, NanMax={np.nanmax(output_tensor):.4f}, \"\n",
    "                    f\"NanMean={np.nanmean(output_tensor):.4f}, NumNaNs={np.isnan(output_tensor).sum()}, \"\n",
    "                    f\"Shape={output_tensor.shape}, Dtype={output_tensor.dtype}\")\n",
    "\n",
    "        # logging.info(f\"Output tensor (antes de nan_to_num): Min={np.nanmin(output_tensor):.4f}, Max={np.nanmax(output_tensor):.4f}, Mean={np.nanmean(output_tensor):.4f}\")\n",
    "\n",
    "        # **AJUSTE CLAVE PARA MANEJO DE NaN EN LA PÉRDIDA**\n",
    "        # Para x (entrada del modelo), los NaN se pueden convertir a 0 (o un valor de imputación)\n",
    "        x = torch.from_numpy(np.nan_to_num(input_tensor, nan=0.0)).float() #\n",
    "        # Para y (objetivo), MANTENER los NaN. La función de pérdida se encargará de enmascararlos.\n",
    "        y = torch.from_numpy(output_tensor).float() # ANTES: np.nan_to_num(output_tensor, nan=0.0)        \n",
    "        # --- Lógica para devolver Timestamps (DEBES IMPLEMENTAR LA EXTRACCIÓN REAL) ---\n",
    "        # last_input_file_path = sequence_files[self.seq_len - 1]\n",
    "        # filename_no_ext = os.path.splitext(os.path.basename(last_input_file_path))[0]\n",
    "        # last_input_dt_utc_placeholder = datetime.utcnow() # ¡ESTO ES SOLO UN PLACEHOLDER!\n",
    "        # try:import torch.optim as optim\n",
    "        #     # Intenta parsear el timestamp del nombre del archivo o del subdirectorio\n",
    "        #     # Ejemplo: parts = filename_no_ext.split('_'); timestamp_str = parts[0][-8:] + parts[1]\n",
    "        #     # last_input_dt_utc = datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")\n",
    "        #     pass # Implementa tu lógica de parseo aquí\n",
    "        # except Exception as e_time:\n",
    "        #     logging.warning(f\"No se pudo parsear el timestamp de {last_input_file_path} en dataset. Usando placeholder. Error: {e_time}\")\n",
    "        #     # last_input_dt_utc = last_input_dt_utc_placeholder # Mantener el placeholder si falla\n",
    "    \n",
    "        # return x, y, last_input_dt_utc_placeholder # Si devuelves timestamp\n",
    "\n",
    "        logging.debug(f\"RadarDataset: output_tensor (con NaNs) stats: \"\n",
    "              f\"NanMin={np.nanmin(output_tensor) if np.isnan(output_tensor).any() else np.min(output_tensor):.4f}, \"\n",
    "              f\"NanMax={np.nanmax(output_tensor) if np.isnan(output_tensor).any() else np.max(output_tensor):.4f}, \"\n",
    "              f\"NanMean={np.nanmean(output_tensor):.4f}, \"\n",
    "              f\"NumNaNs={np.isnan(output_tensor).sum()}\")\n",
    "        \n",
    "        return x, y # Si NO devuelves timestamp por ahora\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2 # Asegura \"same\" padding\n",
    "        self.bias = bias\n",
    "        # La convolución combina la entrada y el estado oculto anterior.\n",
    "        # Genera las 4 compuertas/candidatos de una vez.\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim, # Para i, f, o, g\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "        \n",
    "        # Inicialización de pesos y sesgos\n",
    "        nn.init.xavier_uniform_(self.conv.weight) # Buena práctica para pesos\n",
    "        if self.bias:\n",
    "            nn.init.zeros_(self.conv.bias) # Práctica estándar para sesgos\n",
    "            # Opcional: Inicializar el sesgo de la compuerta de olvido (forget gate) a un valor pequeño positivo\n",
    "            # Esto puede ayudar a que la celda recuerde información por defecto al inicio del entrenamiento.\n",
    "            # La compuerta de olvido es el segundo bloque de canales.\n",
    "            # self.conv.bias.data[hidden_dim : 2 * hidden_dim].fill_(1.0) \n",
    "\n",
    "    def forward(self, input_tensor, cur_state): # input_tensor: (B, C_in, H, W)\n",
    "        h_cur, c_cur = cur_state # Estados oculto y de celda previos\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1) # Concatena entrada y estado oculto\n",
    "        combined_conv = self.conv(combined) # Aplica la convolución\n",
    "        \n",
    "        # Divide en las 4 partes para las compuertas y el candidato a celda\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1) \n",
    "        \n",
    "        i = torch.sigmoid(cc_i) # Compuerta de entrada (input gate)\n",
    "        f = torch.sigmoid(cc_f) # Compuerta de olvido (forget gate)\n",
    "        o = torch.sigmoid(cc_o) # Compuerta de salida (output gate)\n",
    "        g = torch.tanh(cc_g)    # Candidato a celda (cell candidate)\n",
    "        \n",
    "        c_next = f * c_cur + i * g # Nuevo estado de celda\n",
    "        h_next = o * torch.tanh(c_next) # Nuevo estado oculto\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size, device):\n",
    "        height, width = image_size\n",
    "        # Inicializa los estados oculto y de celda a ceros\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=device))\n",
    "\n",
    "class ConvLSTM2DLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM2DLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers # Importante para capas apiladas\n",
    "        self.cell = ConvLSTMCell(input_dim, hidden_dim, kernel_size, bias) #\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None): # input_tensor: (B, T_in, C_in, H, W)\n",
    "        # B: Batch size, T_in: Sequence length, C_in: Input channels, H: Height, W: Width\n",
    "        b, seq_len, _, h, w = input_tensor.size() # _ es C_in\n",
    "        device = input_tensor.device # Obtener el dispositivo del tensor de entrada\n",
    "        \n",
    "        # Inicializar estado oculto si no se proporciona\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self.cell.init_hidden(b, (h, w), device) #\n",
    "\n",
    "        layer_output_list = []\n",
    "        h_cur, c_cur = hidden_state # Desempaquetar estados actuales\n",
    "\n",
    "        # Iterar a través de la secuencia de tiempo\n",
    "        for t in range(seq_len):\n",
    "            # input_tensor[:, t, :, :, :] tiene forma (B, C_in, H, W)\n",
    "            h_cur, c_cur = self.cell(input_tensor=input_tensor[:, t, :, :, :], cur_state=[h_cur, c_cur]) #\n",
    "            layer_output_list.append(h_cur) # Guardar el estado oculto de este paso de tiempo\n",
    "\n",
    "        if self.return_all_layers:\n",
    "            # Apilar todos los estados ocultos a lo largo de la dimensión de tiempo\n",
    "            layer_output = torch.stack(layer_output_list, dim=1) # Forma: (B, T_in, C_hidden, H, W)\n",
    "        else:\n",
    "            # Devolver solo el último estado oculto, manteniendo una dimensión de tiempo de tamaño 1\n",
    "            layer_output = h_cur.unsqueeze(1) # Forma: (B, 1, C_hidden, H, W)\n",
    "\n",
    "        return layer_output, (h_cur, c_cur) # Devuelve la salida de la capa y el último estado (oculto y de celda)\n",
    "\n",
    "class ConvLSTM3D_Enhanced(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dims=[32, 64], kernel_sizes=[(3,3), (3,3)],\n",
    "                 num_layers=2, pred_steps=1, use_layer_norm=True, use_residual=False, # use_residual no se implementa actualmente\n",
    "                 img_height=500, img_width=500,\n",
    "                 # Añadir para gradient checkpointing\n",
    "                 use_gradient_checkpointing=False): # Nuevo parámetro\n",
    "        super(ConvLSTM3D_Enhanced, self).__init__()\n",
    "        # Asegurar que hidden_dims y kernel_sizes sean listas de la longitud correcta\n",
    "        if isinstance(hidden_dims, int): hidden_dims = [hidden_dims] * num_layers # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "        if isinstance(kernel_sizes, tuple): kernel_sizes = [kernel_sizes] * num_layers # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "        assert len(hidden_dims) == num_layers and len(kernel_sizes) == num_layers, \\\n",
    "               \"hidden_dims y kernel_sizes deben tener una longitud igual a num_layers\"\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.num_layers = num_layers\n",
    "        self.pred_steps = pred_steps # Número de pasos de tiempo a predecir\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.use_residual = use_residual # Actualmente no implementado en el forward\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.use_gradient_checkpointing = use_gradient_checkpointing # Guardar para usar en forward\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList() if use_layer_norm else None\n",
    "\n",
    "        current_dim = input_dim\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(\n",
    "                ConvLSTM2DLayer(input_dim=current_dim, hidden_dim=hidden_dims[i],\n",
    "                                kernel_size=kernel_sizes[i], bias=True,\n",
    "                                # La última capa ConvLSTM2D devuelve solo el último estado oculto (con T=1)\n",
    "                                # Las capas intermedias devuelven toda la secuencia de estados ocultos\n",
    "                                return_all_layers=True if i < num_layers - 1 else False) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "            )\n",
    "            if use_layer_norm:\n",
    "                # LayerNorm se aplica a (C, H, W)\n",
    "                self.layer_norms.append(nn.LayerNorm([hidden_dims[i], img_height, img_width])) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "            current_dim = hidden_dims[i]\n",
    "\n",
    "        # Capa convolucional de salida para mapear el último estado oculto a la predicción\n",
    "        # El kernel_size=(1, 3, 3) opera sobre la dimensión temporal (que es 1 para la salida de la última ConvLSTM2DLayer)\n",
    "        # y espacialmente.\n",
    "        self.output_conv = nn.Conv3d(in_channels=hidden_dims[-1],\n",
    "                                     out_channels=input_dim * pred_steps, # Predice `pred_steps` para cada canal de entrada\n",
    "                                     kernel_size=(1, 3, 3), padding=(0, 1, 1)) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "        self.sigmoid = nn.Sigmoid() # Para asegurar salida en [0,1] (normalizada) [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "\n",
    "        # Inicialización de pesos\n",
    "        nn.init.xavier_uniform_(self.output_conv.weight) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "        if self.output_conv.bias is not None:\n",
    "            nn.init.zeros_(self.output_conv.bias) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "\n",
    "        logging.info(f\"Modelo ConvLSTM3D_Enhanced creado: {num_layers} capas, Hidden dims: {hidden_dims}, LayerNorm: {use_layer_norm}, PredSteps: {pred_steps}\") # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "\n",
    "    def forward(self, x_volumetric):  # Espera (Z, B, T_in, H, W, C_in)\n",
    "        num_z_levels, b, seq_len, h, w, c_in = x_volumetric.shape # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "        all_level_predictions = []\n",
    "\n",
    "        # Procesar cada nivel Z de forma independiente\n",
    "        for z_idx in range(num_z_levels):\n",
    "            x_level = x_volumetric[z_idx, ...]  # Forma: (B, T_in, H, W, C_in) [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "            # Permutar para ConvLSTM2DLayer: (B, T_in, C_in, H, W)\n",
    "            x_level_permuted = x_level.permute(0, 1, 4, 2, 3)  # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "            current_input = x_level_permuted\n",
    "\n",
    "            # Estados ocultos para este nivel Z (no se comparten entre niveles Z)\n",
    "            hidden_states_for_level = [None] * self.num_layers # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                # Aplicar Gradient Checkpointing si está habilitado y en modo entrenamiento\n",
    "                if self.use_gradient_checkpointing and self.training:\n",
    "                    # Es importante que los argumentos que no requieren gradiente (como hidden_state si es None)\n",
    "                    # no causen problemas con checkpoint. PyTorch maneja esto bien.\n",
    "                    # `use_reentrant=False` es recomendado para versiones más nuevas de PyTorch.\n",
    "                    layer_output, hidden_state = torch.utils.checkpoint.checkpoint(\n",
    "                        self.layers[i], current_input, hidden_states_for_level[i],\n",
    "                        use_reentrant=False \n",
    "                    )\n",
    "                else:\n",
    "                    layer_output, hidden_state = self.layers[i](current_input, hidden_states_for_level[i]) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "                \n",
    "                hidden_states_for_level[i] = hidden_state # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "\n",
    "                if self.use_layer_norm and self.layer_norms:\n",
    "                    # LayerNorm espera (N, C, H, W) o (N, ..., C)\n",
    "                    # La salida de ConvLSTM2DLayer es (B, T, C_hidden, H, W)\n",
    "                    B_ln, T_ln, C_ln, H_ln, W_ln = layer_output.shape # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "                    # Reshape para LayerNorm: (B*T, C_hidden, H, W)\n",
    "                    output_reshaped_for_ln = layer_output.contiguous().view(B_ln * T_ln, C_ln, H_ln, W_ln) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "                    normalized_output = self.layer_norms[i](output_reshaped_for_ln) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "                    layer_output = normalized_output.view(B_ln, T_ln, C_ln, H_ln, W_ln) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "                \n",
    "                # Aquí se podría añadir la conexión residual si self.use_residual es True\n",
    "                # if self.use_residual and current_input.shape == layer_output.shape:\n",
    "                #    current_input = current_input + layer_output \n",
    "                # else: (manejar si las dimensiones no coinciden, ej. con una conv 1x1 en current_input)\n",
    "                current_input = layer_output # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "\n",
    "            # La salida de la última capa ConvLSTM2DLayer (si return_all_layers=False) es (B, 1, C_hidden, H, W)\n",
    "            # Permutar para Conv3D: (B, C_hidden, T=1, H, W)\n",
    "            output_for_conv3d = current_input.permute(0, 2, 1, 3, 4) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "            raw_conv_output = self.output_conv(output_for_conv3d) # Salida: (B, C_out*T_pred, 1, H, W) [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "            \n",
    "            # Eliminar la dimensión temporal espuria (T=1) de la salida de Conv3D\n",
    "            prediction_features = raw_conv_output.squeeze(2) # Forma: (B, C_out*T_pred, H, W) [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "            \n",
    "            # Reshape para separar pred_steps y canales de salida\n",
    "            # C_out es self.input_dim (ya que predecimos la misma variable de entrada)\n",
    "            level_prediction = prediction_features.view(b, self.pred_steps, self.input_dim, h, w) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "            \n",
    "            # Permutar a (B, T_pred, H, W, C_out) para consistencia con la forma de y_true\n",
    "            level_prediction = level_prediction.permute(0, 1, 3, 4, 2) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "            level_prediction = self.sigmoid(level_prediction) # Aplicar Sigmoid para normalizar a [0,1] [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "            all_level_predictions.append(level_prediction)\n",
    "\n",
    "        # Apilar las predicciones de todos los niveles Z: (Z, B, T_pred, H, W, C_out)\n",
    "        predictions_volumetric = torch.stack(all_level_predictions, dim=0) # [cite: 010625_seq_size_7_2010_2015.ipynb]\n",
    "        return predictions_volumetric\n",
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, data_range=1.0, kernel_size_for_metric=7):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        # torchmetrics.SSIM calcula el Structural Similarity Index Measure.\n",
    "        # data_range: Rango de los datos de entrada (tus predicciones y objetivos normalizados están en [0,1]).\n",
    "        # kernel_size: Tamaño del kernel Gaussiano para la ventana.\n",
    "        # reduction: 'elementwise_mean' promedia los valores SSIM de todas las imágenes en el lote.\n",
    "        self.ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure(\n",
    "            data_range=data_range, #\n",
    "            kernel_size=kernel_size_for_metric, #\n",
    "            reduction='elementwise_mean' # \n",
    "        ).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) #\n",
    "\n",
    "    def forward(self, predictions, targets): \n",
    "        # Renombrado img1 a predictions, img2 a targets para mayor claridad.\n",
    "        # Ambas se esperan con forma (Z, B, T_pred, H, W, C)\n",
    "        # 'targets' puede contener NaNs (gracias a los cambios en RadarDataset).\n",
    "        # 'predictions' debería ser la salida del modelo (ya normalizada por Sigmoid, sin NaNs).\n",
    "\n",
    "        num_z, batch_s, pred_t, height, width, channels = predictions.shape #\n",
    "\n",
    "        # SSIM se aplica a imágenes 2D (o slices con canales).\n",
    "        # Aplanamos las dimensiones Z, B, T_pred en una sola dimensión de \"lote_aplanado\"\n",
    "        # y permutamos para que la forma sea (lote_aplanado, Canales, H, W), que es lo que espera torchmetrics.\n",
    "        \n",
    "        # (Z, B, T, C, H, W) -> (Z*B*T, C, H, W)\n",
    "        predictions_reshaped = predictions.permute(0, 1, 2, 5, 3, 4).contiguous().view(-1, channels, height, width) #\n",
    "        targets_reshaped = targets.permute(0, 1, 2, 5, 3, 4).contiguous().view(-1, channels, height, width) #\n",
    "\n",
    "        # **AJUSTE CLAVE PARA MANEJAR NaN EN TARGETS PARA SSIM**\n",
    "        # torchmetrics.SSIM no maneja NaNs directamente. Si hay NaNs, el resultado puede ser NaN.\n",
    "        # Para esta pérdida SSIM, una estrategia es imputar los NaNs en los targets.\n",
    "        # Usamos 0.0 para la imputación, ya que en el espacio normalizado [0,1], 0.0 \n",
    "        # corresponde al valor físico mínimo (ej. -30.5 dBZ o el self.min_dbz_norm).\n",
    "        # Las predicciones del modelo (predictions_reshaped) no deberían tener NaNs si vienen de un Sigmoid.\n",
    "        \n",
    "        targets_for_ssim = torch.nan_to_num(targets_reshaped, nan=0.0) \n",
    "        # Opcionalmente, si crees que las predicciones podrían tener NaN por alguna razón extrema:\n",
    "        # predictions_for_ssim = torch.nan_to_num(predictions_reshaped, nan=0.0)\n",
    "        # Pero generalmente, solo imputar targets es suficiente si la salida del modelo es 'limpia'.\n",
    "\n",
    "        # Calcular SSIM. Como reduction='elementwise_mean', esto ya devuelve un scalar tensor con el promedio.\n",
    "        ssim_val_mean = self.ssim_metric(predictions_reshaped, targets_for_ssim) #\n",
    "        \n",
    "        # La línea original `ssim_val_mean = ssim_val_elementwise.mean()` es redundante aquí\n",
    "        # si `reduction` en el constructor ya es 'elementwise_mean'.\n",
    "\n",
    "        logging.info(f\"SSIM Mean (targets con NaNs imputados a 0.0): {ssim_val_mean.item():.4f}\") #\n",
    "        \n",
    "        # SSIM es una medida de similitud (mayor es mejor, rango [-1, 1] o [0, 1] para imágenes no negativas).\n",
    "        # Para usarlo como pérdida (menor es mejor), se usa 1 - SSIM.\n",
    "        return 1.0 - ssim_val_mean #\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #\n",
    "    logging.info(f\"Usando dispositivo: {device}\") #\n",
    "    model.to(device) #\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config.get('weight_decay', 1e-5)) #\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=config.get('lr_patience', 3), verbose=True) #\n",
    "\n",
    "    # MSELoss con reducción 'mean' es el estándar. Lo aplicaremos a tensores ya filtrados.\n",
    "    criterion_mse = nn.MSELoss().to(device) #\n",
    "    criterion_ssim = None #\n",
    "    ssim_loss_weight = 0.0 #\n",
    "    mse_loss_weight = 1.0 #\n",
    "\n",
    "    if config.get('use_ssim_loss', False): #\n",
    "        try:\n",
    "            criterion_ssim = SSIMLoss( #\n",
    "                data_range=1.0, \n",
    "                kernel_size_for_metric=config.get('ssim_kernel_size', 7)\n",
    "            ).to(device)\n",
    "            ssim_loss_weight = config.get('ssim_loss_weight', 0.3) #\n",
    "            mse_loss_weight = 1.0 - ssim_loss_weight #\n",
    "            logging.info(f\"Usando SSIM loss con peso {ssim_loss_weight} y MSE con peso {mse_loss_weight}\") #\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al inicializar SSIMLoss: {e}. Se usará solo MSE.\") #\n",
    "            criterion_ssim = None \n",
    "            ssim_loss_weight = 0.0\n",
    "            mse_loss_weight = 1.0\n",
    "\n",
    "    scaler = torch.amp.GradScaler(enabled=config['use_amp']) #\n",
    "\n",
    "    best_val_loss = float('inf') #\n",
    "    train_losses, val_losses = [], [] #\n",
    "    accumulation_steps = config.get('accumulation_steps', 1) #\n",
    "\n",
    "    logging.info(f\"Iniciando entrenamiento: {config['epochs']} épocas, LR: {config['learning_rate']}, Batch (efectivo): {config['batch_size'] * accumulation_steps}\") #\n",
    "\n",
    "    for epoch in range(config['epochs']): #\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache() #\n",
    "        epoch_start_time = time.time() #\n",
    "        model.train() #\n",
    "        running_train_loss = 0.0 #\n",
    "        optimizer.zero_grad() #\n",
    "        \n",
    "        if torch.cuda.is_available(): #\n",
    "            logging.info(f\"Inicio Época {epoch+1} - Memoria GPU Asignada: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB, Reservada: {torch.cuda.memory_reserved(device) / 1024**3:.2f} GB\") # [cite: 7]\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader): # y ahora contiene NaNs\n",
    "            x = x.to(device) #\n",
    "            y = y.to(device) # y tiene NaNs\n",
    "\n",
    "            if x.dim() == 6 and y.dim() == 6:  # (B,Z,T_in,H,W,C) y (B,Z,T_out,H,W,C)\n",
    "                x = x.permute(1, 0, 2, 3, 4, 5) # (Z, B, T_in, H, W, C)\n",
    "                y = y.permute(1, 0, 2, 3, 4, 5) # (Z, B, T_out, H, W, C)\n",
    "            else:\n",
    "                logging.error(f\"Formas inesperadas para x o y antes de la permutación: x={x.shape}, y={y.shape}\") #\n",
    "                continue\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=config['use_amp']): #\n",
    "                predictions = model(x) #\n",
    "\n",
    "                if predictions.shape != y.shape: #\n",
    "                    logging.error(f\"Discrepancia de formas entre predicción {predictions.shape} y objetivo {y.shape}\") #\n",
    "                    continue\n",
    "\n",
    "                # **AJUSTE CLAVE PARA MSE CON NaN EN Y**\n",
    "                # 1. Crear una máscara para los píxeles válidos (no-NaN) en el tensor objetivo 'y'\n",
    "                valid_pixel_mask = ~torch.isnan(y)\n",
    "\n",
    "                # 2. Calcular MSE solo sobre los píxeles válidos\n",
    "                if valid_pixel_mask.sum() > 0:  # Asegurarse de que hay al menos un píxel válido\n",
    "                    # Aplicar la máscara tanto a las predicciones como a los objetivos\n",
    "                    # Esto crea tensores 1D con solo los elementos válidos\n",
    "                    loss_mse_val = criterion_mse(predictions[valid_pixel_mask], y[valid_pixel_mask])\n",
    "                else:\n",
    "                    # Si (muy improbablemente) todos los píxeles en 'y' son NaN para este lote\n",
    "                    loss_mse_val = torch.tensor(0.0, device=device, requires_grad=predictions.requires_grad if predictions.requires_grad else False)\n",
    "                current_loss = loss_mse_val #\n",
    "\n",
    "                if criterion_ssim is not None: #\n",
    "                    # SSIMLoss ahora maneja NaNs en 'y' internamente (imputándolos)\n",
    "                    loss_ssim_component = criterion_ssim(predictions, y) #\n",
    "                    current_loss = mse_loss_weight * loss_mse_val + ssim_loss_weight * loss_ssim_component #\n",
    "\n",
    "                if batch_idx == 0 and epoch == 0: #\n",
    "                    if y.numel() > 0:\n",
    "                        is_nan_present_y = torch.isnan(y).any()\n",
    "                        log_nanmin_y, log_nanmax_y, log_nanmean_y = float('nan'), float('nan'), float('nan') # Defaults\n",
    "                        if is_nan_present_y:\n",
    "                            valid_y_elements = y[~torch.isnan(y)]\n",
    "                            if valid_y_elements.numel() > 0:\n",
    "                                log_nanmin_y = valid_y_elements.min().item()\n",
    "                                log_nanmax_y = valid_y_elements.max().item()\n",
    "                                log_nanmean_y = valid_y_elements.mean().item()\n",
    "                        elif y.numel() > 0: # No NaNs, but tensor is not empty\n",
    "                            log_nanmin_y = y.min().item()\n",
    "                            log_nanmax_y = y.max().item()\n",
    "                            log_nanmean_y = y.mean().item()\n",
    "                        \n",
    "                        logging.info(f\"  Objetivos y (normalizados, con NaNs): \"\n",
    "                                    f\"MinVal={log_nanmin_y:.4f}, MaxVal={log_nanmax_y:.4f}, \"\n",
    "                                    f\"MeanVal={log_nanmean_y:.4f}, NumNaNs={torch.isnan(y).sum()}\")\n",
    "                    else:\n",
    "                        logging.info(\"  Objetivos y (normalizados, con NaNs): Tensor vacío.\")\n",
    "                    logging.info(f\"  MSE (masked): {loss_mse_val.item():.6f}\")\n",
    "                    if criterion_ssim is not None:\n",
    "                        logging.info(f\"  SSIM (1-SSIM, imputed targets): {loss_ssim_component.item():.6f}\")\n",
    "\n",
    "\n",
    "                loss_to_accumulate = current_loss / accumulation_steps #\n",
    "\n",
    "            scaler.scale(loss_to_accumulate).backward() #\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader): #\n",
    "                if config.get('clip_grad_norm', None): #\n",
    "                    scaler.unscale_(optimizer) \n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['clip_grad_norm']) #\n",
    "                scaler.step(optimizer) #\n",
    "                scaler.update() #\n",
    "                optimizer.zero_grad() #\n",
    "\n",
    "            running_train_loss += current_loss.item() #\n",
    "\n",
    "            if (batch_idx + 1) % config.get('log_interval', 1) == 0: #\n",
    "                logging.info(f\"Época {epoch+1}/{config['epochs']} [{batch_idx+1}/{len(train_loader)}] - Pérdida (batch): {current_loss.item():.6f}\") #\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader) #\n",
    "        train_losses.append(avg_train_loss) #\n",
    "\n",
    "        # Validación\n",
    "        if val_loader and len(val_loader) > 0: #\n",
    "            model.eval() #\n",
    "            running_val_loss = 0.0 #\n",
    "            with torch.no_grad(): #\n",
    "                for x_val, y_val in val_loader: # y_val ahora contiene NaNs\n",
    "                    x_val = x_val.to(device) #\n",
    "                    y_val = y_val.to(device) #\n",
    "                    if x_val.dim() == 6 and y_val.dim() == 6: #\n",
    "                        x_val = x_val.permute(1, 0, 2, 3, 4, 5) #\n",
    "                        y_val = y_val.permute(1, 0, 2, 3, 4, 5) #\n",
    "                    else:\n",
    "                        logging.error(f\"Formas inesperadas (val) x={x_val.shape}, y={y_val.shape}\") #\n",
    "                        continue\n",
    "\n",
    "                    with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=config['use_amp']): #\n",
    "                        predictions_val = model(x_val) #\n",
    "                        if predictions_val.shape != y_val.shape: #\n",
    "                            logging.error(f\"Discrepancia de formas (val) entre predicción {predictions_val.shape} y objetivo {y_val.shape}\") #\n",
    "                            continue\n",
    "                        \n",
    "                        # **AJUSTE CLAVE PARA MSE CON NaN EN Y_VAL**\n",
    "                        valid_pixel_mask_val = ~torch.isnan(y_val)\n",
    "                        if valid_pixel_mask_val.sum() > 0:\n",
    "                            val_loss_mse_val = criterion_mse(predictions_val[valid_pixel_mask_val], y_val[valid_pixel_mask_val])\n",
    "                        else:\n",
    "                            val_loss_mse_val = torch.tensor(0.0, device=device)\n",
    "\n",
    "                        current_val_loss = val_loss_mse_val #\n",
    "                        if criterion_ssim is not None: #\n",
    "                            val_loss_ssim_component = criterion_ssim(predictions_val, y_val) #\n",
    "                            current_val_loss = mse_loss_weight * val_loss_mse_val + ssim_loss_weight * val_loss_ssim_component #\n",
    "                    running_val_loss += current_val_loss.item() #\n",
    "\n",
    "            if len(val_loader) > 0: #\n",
    "                avg_val_loss = running_val_loss / len(val_loader) #\n",
    "                val_losses.append(avg_val_loss) #\n",
    "                scheduler.step(avg_val_loss) #\n",
    "                epoch_duration = time.time() - epoch_start_time #\n",
    "                logging.info(f\"Época {epoch+1} completada en {epoch_duration:.2f}s. Pérdida (train): {avg_train_loss:.6f}, Pérdida (val): {avg_val_loss:.6f}\") # [cite: 15]\n",
    "\n",
    "                if avg_val_loss < best_val_loss: #\n",
    "                    best_val_loss = avg_val_loss #\n",
    "                    torch.save({'epoch': epoch + 1, 'model_state_dict': model.state_dict(), #\n",
    "                                'optimizer_state_dict': optimizer.state_dict(), 'loss': best_val_loss}, #\n",
    "                               os.path.join(config['model_save_dir'], \"best_convlstm_model.pth\")) #\n",
    "                    logging.info(f\"Mejor modelo guardado (Pérdida Val: {best_val_loss:.6f})\") # [cite: 15]\n",
    "            else: \n",
    "                epoch_duration = time.time() - epoch_start_time #\n",
    "                logging.info(f\"Época {epoch+1} completada en {epoch_duration:.2f}s. Pérdida (train): {avg_train_loss:.6f} (Dataset de validación vacío, no se calculó pérdida de validación)\") #\n",
    "        else: \n",
    "            epoch_duration = time.time() - epoch_start_time #\n",
    "            logging.info(f\"Época {epoch+1} completada en {epoch_duration:.2f}s. Pérdida (train): {avg_train_loss:.6f} (No hay val_loader)\") #\n",
    "\n",
    "        if (epoch + 1) % config.get('checkpoint_interval', 1) == 0: #\n",
    "            torch.save({'epoch': epoch + 1, 'model_state_dict': model.state_dict(), #\n",
    "                        'optimizer_state_dict': optimizer.state_dict(), 'train_losses': train_losses, #\n",
    "                        'val_losses': val_losses if (val_loader and len(val_loader) > 0) else []}, #\n",
    "                       os.path.join(config['model_save_dir'], f\"checkpoint_epoch_{epoch+1}.pth\")) #\n",
    "            logging.info(f\"Checkpoint guardado en la época {epoch+1}\") # [cite: 15]\n",
    "\n",
    "    logging.info(\"Entrenamiento finalizado.\") # [cite: 23]\n",
    "    if train_loader and len(train_losses) > 0: #\n",
    "        plt.figure(figsize=(10, 5)) #\n",
    "        plt.plot(train_losses, label='Pérdida Entrenamiento') #\n",
    "        if val_loader and len(val_losses) > 0: #\n",
    "            plt.plot(val_losses, label='Pérdida Validación') #\n",
    "        plt.xlabel('Épocas') #\n",
    "        plt.ylabel('Pérdida') #\n",
    "        plt.legend() #\n",
    "        plt.title('Curvas de Pérdida del Entrenamiento') #\n",
    "        plt.savefig(os.path.join(config['model_save_dir'], \"loss_curves.png\")) #\n",
    "        plt.close() #\n",
    "        logging.info(f\"Curvas de pérdida guardadas en {os.path.join(config['model_save_dir'], 'loss_curves.png')}\") # [cite: 24]\n",
    "\n",
    "    return model, {'train_losses': train_losses, 'val_losses': val_losses if (val_loader and len(val_loader) > 0) else []} #\n",
    "\n",
    "def generate_prediction_netcdf(model, data_loader, config, device, num_samples=1):\n",
    "    model.to(device) #\n",
    "    model.float() #\n",
    "    model.eval() #\n",
    "\n",
    "    output_dir = config['predictions_output_dir'] #\n",
    "\n",
    "    # **AJUSTE 1: Usar los mismos nombres de config para el rango de (de)normalización**\n",
    "    # Estos deben ser los mismos valores que se usan en RadarDataset.\n",
    "    # Asegúrate de que tu dict 'config' tenga 'norm_min_dbz' y 'norm_max_dbz'.\n",
    "    min_dbz_norm_config = config.get('norm_min_dbz', -30.0) # Valor por defecto si no está en config\n",
    "    max_dbz_norm_config = config.get('norm_max_dbz', 70.0)  # Valor por defecto si no está en config\n",
    "    \n",
    "    output_scale_factor = np.float32(config.get('output_nc_scale_factor', 0.5)) #\n",
    "    output_add_offset = np.float32(config.get('output_nc_add_offset', 33.5)) #\n",
    "    output_fill_value_byte = np.int8(config.get('output_nc_fill_value_byte', -128)) #\n",
    "    \n",
    "    # Umbral físico para considerar una predicción como \"nula\" o \"sin reflectividad\"\n",
    "    # Este valor debería ser cercano a min_dbz_norm_config.\n",
    "    # Por ejemplo, si min_dbz_norm_config es -30.5, un umbral de -28 podría ser adecuado.\n",
    "    # O, si el modelo predice 0.0 dBZ para el fondo, un umbral de 0.1 dBZ.\n",
    "    physical_null_threshold = config.get('physical_null_threshold', min_dbz_norm_config + 2.0) # Ejemplo: -30.5 + 2.0 = -28.5 dBZ\n",
    "    logging.info(f\"Usando umbral físico para nulos: < {physical_null_threshold:.2f} dBZ\")\n",
    "\n",
    "    # Nombres de dimensiones y variables (ya bien parametrizados desde config)\n",
    "    time_dim_name = config.get('time_dim_name', 'time') #\n",
    "    # ... (resto de los nombres de variables y dimensiones) ...\n",
    "    dbz_var_name = config.get('dbz_variable_name_pred_nc', 'DBZ') #\n",
    "    grid_mapping_var_name = config.get('projection_variable_name', \"grid_mapping_0\") #\n",
    "\n",
    "    proj_origin_lon = config.get('sensor_longitude', -68.0169982910156) #\n",
    "    proj_origin_lat = config.get('sensor_latitude', -34.6479988098145) #\n",
    "    earth_radius_m = config.get('earth_radius_m', 6378137) #\n",
    "    \n",
    "    if not os.path.exists(output_dir): os.makedirs(output_dir) #\n",
    "\n",
    "    num_z, num_y, num_x = config['expected_shape'] #\n",
    "\n",
    "    z_coord_values = np.arange(config['grid_minz_km'], config['grid_minz_km'] + num_z * config['grid_dz_km'], config['grid_dz_km'], dtype=np.float32)[:num_z] #\n",
    "    x_coord_values = np.arange(config['grid_minx_km'], config['grid_minx_km'] + num_x * config['grid_dx_km'], config['grid_dx_km'], dtype=np.float32)[:num_x] #\n",
    "    y_coord_values = np.arange(config['grid_miny_km'], config['grid_miny_km'] + num_y * config['grid_dy_km'], config['grid_dy_km'], dtype=np.float32)[:num_y] #\n",
    "    \n",
    "    proj = pyproj.Proj(proj=\"aeqd\", lon_0=proj_origin_lon, lat_0=proj_origin_lat, R=earth_radius_m) #\n",
    "    x_grid, y_grid = np.meshgrid(x_coord_values, y_coord_values) #\n",
    "    lon0, lat0 = proj(x_grid * 1000, y_grid * 1000, inverse=True) # pyproj espera metros para x,y en proyecciones\n",
    "                                                                 # si x_coord_values están en km.\n",
    "\n",
    "    sample_count = 0\n",
    "    with torch.no_grad(): #\n",
    "        for batch_idx, data_batch in enumerate(data_loader): #\n",
    "            if sample_count >= num_samples: break #\n",
    "            \n",
    "            # Asumimos que data_batch puede contener (x, y_con_nans) o (x, y_con_nans, timestamp_del_ultimo_input)\n",
    "            # Por ahora, la lógica del timestamp sigue siendo placeholder si no se modifica RadarDataset\n",
    "            if len(data_batch) == 2:\n",
    "                x_input_volume, _ = data_batch # y_true_volume no se usa aquí para la predicción\n",
    "                # Placeholder para timestamp si no viene del DataLoader\n",
    "                last_input_dt_actual = datetime.utcnow() - timedelta(minutes=config['seq_len'] * config['prediction_interval_minutes'])\n",
    "            elif len(data_batch) == 3: # Suponiendo que el tercer elemento es el timestamp\n",
    "                x_input_volume, _, last_input_dt_actual = data_batch\n",
    "                if not isinstance(last_input_dt_actual, datetime): # Tomar el primero del batch si es una lista/tensor\n",
    "                    last_input_dt_actual = last_input_dt_actual[0] \n",
    "            else:\n",
    "                logging.error(\"Formato de data_batch inesperado.\")\n",
    "                continue\n",
    "\n",
    "            x_permuted = x_input_volume.permute(1, 0, 2, 3, 4, 5) #\n",
    "            x_to_model = x_permuted.to(device) #\n",
    "            \n",
    "            current_x_to_model = x_to_model[:, 0:1, ...] # Procesar solo el primer ítem del batch si batch_size > 1\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=config['use_amp']): #\n",
    "                prediction_norm_all_steps = model(current_x_to_model) #\n",
    "            \n",
    "            # Asumimos pred_steps = 1 para la predicción guardada\n",
    "            pred_data_np = prediction_norm_all_steps[:, 0, 0, :, :, 0].cpu().numpy()  # (Z, H, W)\n",
    "            \n",
    "            # Desnormalizar usando el rango CONSISTENTE\n",
    "            pred_data_desnorm_float = np.where(\n",
    "                np.isnan(pred_data_np), \n",
    "                np.nan,\n",
    "                pred_data_np * (max_dbz_norm_config - min_dbz_norm_config) + min_dbz_norm_config\n",
    "            ) #\n",
    "            \n",
    "            # **AJUSTE 2: Clipping físico explícito (Opcional pero recomendado para robustez)**\n",
    "            pred_data_desnorm_float = np.clip(pred_data_desnorm_float, min_dbz_norm_config, max_dbz_norm_config)\n",
    "            \n",
    "            logging.info(f\"Predicción Física Desnormalizada y Clipeada (muestra {sample_count}): Min={np.nanmin(pred_data_desnorm_float):.2f}, Max={np.nanmax(pred_data_desnorm_float):.2f}, Mean={np.nanmean(pred_data_desnorm_float):.2f}\") #\n",
    "            \n",
    "            # **AJUSTE 3: Marcar áreas sin reflectividad usando el umbral físico configurado**\n",
    "            pred_data_for_packing = np.where(\n",
    "                pred_data_desnorm_float < physical_null_threshold, # Usar el umbral configurable\n",
    "                np.nan, \n",
    "                pred_data_desnorm_float\n",
    "            ) #\n",
    "            \n",
    "            pred_data_byte = np.where(\n",
    "                np.isnan(pred_data_for_packing), \n",
    "                output_fill_value_byte,\n",
    "                np.clip(((pred_data_for_packing - output_add_offset) / output_scale_factor), -127, 127).round().astype(np.int8)\n",
    "            ) #\n",
    "            pred_data_final_for_nc = np.expand_dims(pred_data_byte, axis=0) #\n",
    "\n",
    "            # Lógica de Timestamps (usando last_input_dt_actual)\n",
    "            # Asegúrate que last_input_dt_actual sea un objeto datetime sin tzinfo para estos cálculos\n",
    "            if last_input_dt_actual.tzinfo is not None:\n",
    "                 last_input_dt_actual = last_input_dt_actual.replace(tzinfo=None)\n",
    "\n",
    "            forecast_lead_seconds = (0 + 1) * config['prediction_interval_minutes'] * 60 #\n",
    "            actual_forecast_datetime_utc = last_input_dt_actual + timedelta(seconds=forecast_lead_seconds) #\n",
    "\n",
    "            epoch_time = datetime(1970, 1, 1, 0, 0, 0) #\n",
    "            time_value_seconds = (actual_forecast_datetime_utc - epoch_time).total_seconds() #\n",
    "            \n",
    "            # Estos start/stop time para el NetCDF deberían reflejar el intervalo del dato PREDICHO.\n",
    "            # Por ejemplo, si la predicción es instantánea, start y stop podrían ser iguales al time_value.\n",
    "            # Si representa un intervalo, ajústalo. El original parece usar un intervalo de scan.\n",
    "            # Para una predicción a t+3min, podríamos decir que el intervalo es corto.\n",
    "            time_begin_calc_seconds = time_value_seconds # O un poco antes si representa un intervalo\n",
    "            time_end_calc_seconds = time_value_seconds   #\n",
    "            \n",
    "            file_timestamp_str = actual_forecast_datetime_utc.strftime(\"%Y%m%d_%H%M%S\") #\n",
    "            output_filename = os.path.join(output_dir, f\"pred_{dbz_var_name}_{file_timestamp_str}_sample{sample_count}.nc\") #\n",
    "\n",
    "            with NCDataset(output_filename, 'w', format='NETCDF3_CLASSIC') as ncfile: #\n",
    "                # --- ESCRITURA DE METADATOS Y VARIABLES NetCDF ---\n",
    "                # (Esta parte es extensa y parece mayormente correcta, solo ajustaré los atributos min/max de la variable DBZ)\n",
    "                # ... (copiar toda la sección de creación de dimensiones y variables de tu código original) ...\n",
    "                ncfile.Conventions = \"CF-1.6\" #\n",
    "                ncfile.title = f\"{config.get('radar_name', 'SAN_RAFAEL')} - Forecast t+{config['prediction_interval_minutes']}min\" #\n",
    "                # ... (otros atributos globales) ...\n",
    "                ncfile.references = f\"Tesis de {config.get('author_name', 'Federico Caballero')}, {config.get('author_institution', 'Universidad de Mendoza')}\" #\n",
    "\n",
    "                ncfile.createDimension(time_dim_name, None) #\n",
    "                ncfile.createDimension(config.get('bounds_dim_name', 'bounds'), 2) #\n",
    "                ncfile.createDimension(config.get('x_dim_name', 'x0'), num_x) #\n",
    "                ncfile.createDimension(config.get('y_dim_name', 'y0'), num_y) #\n",
    "                ncfile.createDimension(config.get('z_dim_name', 'z0'), num_z) #\n",
    "\n",
    "                time_v = ncfile.createVariable(config.get('time_var_name', 'time'), 'f8', (time_dim_name,)) #\n",
    "                time_v.standard_name = \"time\" #\n",
    "                time_v.long_name = \"Data time\" #\n",
    "                time_v.units = \"seconds since 1970-01-01T00:00:00Z\" #\n",
    "                time_v.axis = \"T\" #\n",
    "                time_v.bounds = config.get('time_bounds_var_name', 'time_bounds') #\n",
    "                time_v.comment = actual_forecast_datetime_utc.strftime(\"%Y-%m-%dT%H:%M:%SZ\") #\n",
    "                time_v[:] = [time_value_seconds] #\n",
    "\n",
    "                time_bnds_v = ncfile.createVariable(config.get('time_bounds_var_name', 'time_bounds'), 'f8', (time_dim_name, config.get('bounds_dim_name', 'bounds'))) #\n",
    "                time_bnds_v.comment = \"Time bounds for data interval\" #\n",
    "                time_bnds_v.units = \"seconds since 1970-01-01T00:00:00Z\" #\n",
    "                time_bnds_v[:] = [[time_begin_calc_seconds, time_end_calc_seconds]] #\n",
    "                \n",
    "                # ... (definición de start_time_v, stop_time_v, x_v, y_v, z_v, lat0_v, lon0_v, gm_v como en tu código)\n",
    "                # (Asegúrate que estos también se llenen correctamente)\n",
    "                start_time_v = ncfile.createVariable(config.get('start_time_var_name', 'start_time'), 'f8', (time_dim_name,)) #\n",
    "                start_time_v.long_name = \"start_time\" #\n",
    "                start_time_v.units = \"seconds since 1970-01-01T00:00:00Z\" #\n",
    "                start_time_v.comment = datetime.fromtimestamp(time_begin_calc_seconds).strftime(\"%Y-%m-%dT%H:%M:%SZ\") #\n",
    "                start_time_v[:] = [time_begin_calc_seconds] #\n",
    "\n",
    "                stop_time_v = ncfile.createVariable(config.get('stop_time_var_name', 'stop_time'), 'f8', (time_dim_name,)) #\n",
    "                stop_time_v.long_name = \"stop_time\" #\n",
    "                stop_time_v.units = \"seconds since 1970-01-01T00:00:00Z\" #\n",
    "                stop_time_v.comment = datetime.fromtimestamp(time_end_calc_seconds).strftime(\"%Y-%m-%dT%H:%M:%SZ\") #\n",
    "                stop_time_v[:] = [time_end_calc_seconds] #\n",
    "\n",
    "                x_v = ncfile.createVariable(config.get('x_coord_var_name', 'x0'), 'f4', (config.get('x_dim_name', 'x0'),)) #\n",
    "                x_v.standard_name = \"projection_x_coordinate\" #\n",
    "                x_v.units = \"km\" #\n",
    "                x_v.axis = \"X\" #\n",
    "                x_v[:] = x_coord_values #\n",
    "                \n",
    "                y_v = ncfile.createVariable(config.get('y_coord_var_name', 'y0'), 'f4', (config.get('y_dim_name', 'y0'),)) #\n",
    "                y_v.standard_name = \"projection_y_coordinate\" #\n",
    "                y_v.units = \"km\" #\n",
    "                y_v.axis = \"Y\" #\n",
    "                y_v[:] = y_coord_values #\n",
    "                \n",
    "                z_v = ncfile.createVariable(config.get('z_coord_var_name', 'z0'), 'f4', (config.get('z_dim_name', 'z0'),)) #\n",
    "                z_v.standard_name = \"altitude\" #\n",
    "                z_v.long_name = \"constant altitude levels\" #\n",
    "                z_v.units = \"km\" #\n",
    "                z_v.positive = \"up\" #\n",
    "                z_v.axis = \"Z\" #\n",
    "                z_v[:] = z_coord_values #\n",
    "\n",
    "                lat0_v = ncfile.createVariable('lat0', 'f4', (config.get('y_dim_name', 'y0'), config.get('x_dim_name', 'x0'))) #\n",
    "                lat0_v.standard_name = \"latitude\" #\n",
    "                lat0_v.units = \"degrees_north\" #\n",
    "                lat0_v[:] = lat0 #\n",
    "                \n",
    "                lon0_v = ncfile.createVariable('lon0', 'f4', (config.get('y_dim_name', 'y0'), config.get('x_dim_name', 'x0'))) #\n",
    "                lon0_v.standard_name = \"longitude\" #\n",
    "                lon0_v.units = \"degrees_east\" #\n",
    "                lon0_v[:] = lon0 #\n",
    "\n",
    "                gm_v = ncfile.createVariable(grid_mapping_var_name, 'i4') #\n",
    "                gm_v.grid_mapping_name = \"azimuthal_equidistant\" #\n",
    "                gm_v.longitude_of_projection_origin = proj_origin_lon #\n",
    "                gm_v.latitude_of_projection_origin = proj_origin_lat #\n",
    "                gm_v.false_easting = 0.0 #\n",
    "                gm_v.false_northing = 0.0 #\n",
    "                gm_v.earth_radius = earth_radius_m #\n",
    "\n",
    "                dbz_v = ncfile.createVariable(dbz_var_name, 'i1', \n",
    "                             (config.get('time_dim_name', 'time'), \n",
    "                              config.get('z_dim_name', 'z0'), # <<-- OBTENER DE CONFIG\n",
    "                              config.get('y_dim_name', 'y0'), \n",
    "                              config.get('x_dim_name', 'x0')),\n",
    "                             fill_value=output_fill_value_byte) #\n",
    "                dbz_v.units = 'dBZ' #\n",
    "                dbz_v.long_name = 'DBZ' #\n",
    "                dbz_v.standard_name = 'DBZ' #\n",
    "                dbz_v.coordinates = \"lon0 lat0\" #\n",
    "                dbz_v.grid_mapping = grid_mapping_var_name #\n",
    "                dbz_v.scale_factor = output_scale_factor #\n",
    "                dbz_v.add_offset = output_add_offset #\n",
    "                dbz_v.valid_min = np.int8(-127) #\n",
    "                dbz_v.valid_max = np.int8(127) #\n",
    "                # **AJUSTE 4: Atributos min/max_value consistentes**\n",
    "                dbz_v.min_value = np.float32(min_dbz_norm_config) # Usar el mismo min de (de)normalización\n",
    "                dbz_v.max_value = np.float32(max_dbz_norm_config) # Usar el mismo max de (de)normalización\n",
    "                dbz_v[:] = pred_data_final_for_nc #\n",
    "\n",
    "                print(f\"NetCDF predicción t+{config['prediction_interval_minutes']}min guardado: {output_filename}\") #\n",
    "            sample_count += 1 #\n",
    "\n",
    "def main():\n",
    "    set_seed(42) #\n",
    "    config = {\n",
    "        'data_dir': \"/home/sample\", \n",
    "        'model_save_dir': \"/home/model_output_final_v_ckpt\",\n",
    "        'predictions_output_dir': \"/home/predictions_final_v_ckpt\",\n",
    "\n",
    "        'seq_len': 6, \n",
    "        'pred_len': 1, \n",
    "        'pred_steps_model': 1, \n",
    "\n",
    "        # **AJUSTE 1: Estandarizar nombres y valores para normalización/desnormalización**\n",
    "        # Estos serán usados por RadarDataset y generate_prediction_netcdf\n",
    "        'norm_min_dbz': -30.5,  # Valor físico del _FillValue original (-128 * 0.5 + 33.5), o un poco menos (-31).\n",
    "        'norm_max_dbz': 70.0,   # Máximo físico esperado/deseado (ej. 65.0, 70.0, o 75.0)\n",
    "        \n",
    "        # **AJUSTE 2: Umbral físico para considerar una predicción como nula (para _FillValue)**\n",
    "        # Debe ser un poco mayor que norm_min_dbz para capturar predicciones que son \"nulas\".\n",
    "        # Si norm_min_dbz es -30.5, un umbral de -28.0 podría ser un buen inicio.\n",
    "        # Si el modelo predice 0.0 dBZ para el fondo, un umbral de 0.1 dBZ sería adecuado.\n",
    "        'physical_null_threshold': -28.0, # dBZ. Ajusta según lo que el modelo aprenda a predecir para nulos.\n",
    "\n",
    "        # 'fill_value': -9999.0, # Este fill_value en config no se usa activamente si RadarDataset maneja NaNs\n",
    "        'expected_shape': (18, 500, 500), \n",
    "        'dbz_variable_name': 'DBZ', # Para leer de los NC de entrada\n",
    "\n",
    "        # Parámetros del Sensor y Grilla (mayormente para metadatos NetCDF)\n",
    "        'sensor_latitude': -34.64799880981445,   \n",
    "        'sensor_longitude': -68.01699829101562,  \n",
    "        'sensor_altitude_km': 0.550000011920929,\n",
    "        'grid_minz_km': 1.0, 'grid_dz_km': 1.0,\n",
    "        'grid_minx_km': -249.5, 'grid_dx_km': 1.0,\n",
    "        'grid_miny_km': -249.5, 'grid_dy_km': 1.0,\n",
    "        'radar_name': \"SAN_RAFAEL\", 'institution_name': \"UCAR\", \n",
    "        'author_name': \"Federico Caballero\", 'author_institution': \"Universidad de Mendoza\",\n",
    "        'data_source_name': \"Gobierno de Mendoza\", \n",
    "\n",
    "        # Parámetros para la SALIDA NetCDF\n",
    "        'dbz_variable_name_pred_nc': 'DBZ', \n",
    "        'output_nc_scale_factor': 0.5, #\n",
    "        'output_nc_add_offset': 33.5, #\n",
    "        'output_nc_fill_value_byte': -128, # El byte para _FillValue\n",
    "        'projection_variable_name': \"grid_mapping_0\", \n",
    "        'earth_radius_m': 6378137.0, \n",
    "\n",
    "        'prediction_interval_minutes': 3, \n",
    "\n",
    "        # Parámetros del Modelo ConvLSTM\n",
    "        'model_input_dim': 1,\n",
    "        'model_hidden_dims': [32, 32], #\n",
    "        'model_kernel_sizes': [(3,3), (3,3)], #\n",
    "        'model_num_layers': 2, #\n",
    "        'model_use_layer_norm': True, 'model_use_residual': False, #\n",
    "        'use_gradient_checkpointing': True, # **AJUSTE 3: Habilitar/Deshabilitar Gradient Checkpointing**\n",
    "\n",
    "        # Parámetros de Entrenamiento\n",
    "        'batch_size': 1, #\n",
    "        'epochs': 2, # Mantener bajo para pruebas iniciales\n",
    "        'learning_rate': 1e-3, 'weight_decay': 1e-4, 'lr_patience': 3, #\n",
    "        'use_amp': True, 'accumulation_steps': 1, #\n",
    "        'clip_grad_norm': 1.0, #\n",
    "        'log_interval': 1, \n",
    "        'checkpoint_interval': 1, #\n",
    "\n",
    "        'use_ssim_loss': False, 'ssim_kernel_size': 7, 'ssim_loss_weight': 0.3, #\n",
    "\n",
    "        'train_val_split_ratio': 0.8, #\n",
    "        'max_sequences_to_use': 10, # Para pruebas rápidas\n",
    "    }\n",
    "\n",
    "    os.makedirs(config['model_save_dir'], exist_ok=True) #\n",
    "    os.makedirs(config['predictions_output_dir'], exist_ok=True) #\n",
    "\n",
    "    all_subdirs_available = sorted([ #\n",
    "        d for d in os.listdir(config['data_dir'])\n",
    "        if os.path.isdir(os.path.join(config['data_dir'], d)) and not d.startswith('.')\n",
    "    ])\n",
    "    if not all_subdirs_available: logging.error(f\"No subdirs in {config['data_dir']}\"); return #\n",
    "\n",
    "    if config.get('max_sequences_to_use') and config['max_sequences_to_use'] < len(all_subdirs_available): # Usar .get para seguridad\n",
    "        logging.info(f\"Usando muestra aleatoria de {config['max_sequences_to_use']} secuencias.\") #\n",
    "        random.shuffle(all_subdirs_available) #\n",
    "        subdirs_to_use = all_subdirs_available[:config['max_sequences_to_use']] #\n",
    "    else: subdirs_to_use = all_subdirs_available #\n",
    "    \n",
    "    logging.info(f\"Total secuencias a usar: {len(subdirs_to_use)}.\") #\n",
    "    if not subdirs_to_use : logging.error(\"No hay secuencias para procesar.\"); return #\n",
    "\n",
    "    split_idx = int(len(subdirs_to_use) * config['train_val_split_ratio']) #\n",
    "    train_subdirs, val_subdirs = subdirs_to_use[:split_idx], subdirs_to_use[split_idx:] #\n",
    "    if not train_subdirs: logging.info(\"No hay secuencias de entrenamiento, usando todas para validación si existen.\"); train_subdirs = [] #\n",
    "    logging.info(f\"Entrenamiento: {len(train_subdirs)} sec. Validación: {len(val_subdirs)} sec.\") #\n",
    "\n",
    "    train_loader = None #\n",
    "    if train_subdirs: #\n",
    "        # **AJUSTE 4: Pasar los parámetros de normalización correctos a RadarDataset**\n",
    "        train_dataset = RadarDataset(config['data_dir'], train_subdirs, \n",
    "                                     seq_len=config['seq_len'], pred_len=config['pred_len'],\n",
    "                                     min_dbz_norm=config['norm_min_dbz'], # Usar nuevo nombre de config\n",
    "                                     max_dbz_norm=config['norm_max_dbz'], # Usar nuevo nombre de config\n",
    "                                     expected_shape=config['expected_shape'], \n",
    "                                     variable_name=config['dbz_variable_name'])\n",
    "        if len(train_dataset) > 0: #\n",
    "            train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True) #\n",
    "        else:\n",
    "            logging.info(\"Dataset de entrenamiento vacío después de filtrar.\") #\n",
    "\n",
    "    val_loader = None; val_dataset_len = 0 #\n",
    "    if val_subdirs: #\n",
    "        val_dataset = RadarDataset(config['data_dir'], val_subdirs, \n",
    "                                   seq_len=config['seq_len'], pred_len=config['pred_len'],\n",
    "                                   min_dbz_norm=config['norm_min_dbz'], # Usar nuevo nombre de config\n",
    "                                   max_dbz_norm=config['norm_max_dbz'], # Usar nuevo nombre de config\n",
    "                                   expected_shape=config['expected_shape'], \n",
    "                                   variable_name=config['dbz_variable_name'])\n",
    "        val_dataset_len = len(val_dataset) #\n",
    "        if val_dataset_len > 0: #\n",
    "             val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True) #\n",
    "        else: logging.info(\"Dataset de validación vacío.\") #\n",
    "    else: logging.info(\"No subdirectorios para validación.\") #\n",
    "\n",
    "    if not val_loader and not train_loader: #\n",
    "        logging.error(\"No hay datos de validación ni de entrenamiento para generar predicciones.\") #\n",
    "        return\n",
    "\n",
    "    model = ConvLSTM3D_Enhanced( #\n",
    "        input_dim=config['model_input_dim'], hidden_dims=config['model_hidden_dims'],\n",
    "        kernel_sizes=config['model_kernel_sizes'], num_layers=config['model_num_layers'],\n",
    "        pred_steps=config['pred_steps_model'], use_layer_norm=config['model_use_layer_norm'],\n",
    "        use_residual=config['model_use_residual'],\n",
    "        img_height=config['expected_shape'][1], img_width=config['expected_shape'][2],\n",
    "        use_gradient_checkpointing=config.get('use_gradient_checkpointing', False) # **AJUSTE 5**\n",
    "    )\n",
    "    model.float() #\n",
    "\n",
    "    logging.info(f\"Arquitectura del modelo:\\n{model}\") #\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) #\n",
    "    logging.info(f\"Número total de parámetros entrenables: {total_params:,}\") #\n",
    "\n",
    "    device_for_execution = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #\n",
    "\n",
    "    model_path = os.path.join(config['model_save_dir'], \"best_convlstm_model.pth\") #\n",
    "    if os.path.exists(model_path) and config.get('load_existing_model_if_available', True): # Nuevo flag para controlar carga\n",
    "        logging.info(f\"Cargando modelo pre-entrenado desde: {model_path}\") #\n",
    "        try:\n",
    "            # Cargar a CPU primero para evitar problemas de GPU si el modelo se guardó en una GPU diferente\n",
    "            checkpoint_data = torch.load(model_path, map_location='cpu') \n",
    "            model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "            model.float() \n",
    "            # Opcional: Cargar estado del optimizador si vas a continuar el entrenamiento\n",
    "            # if 'optimizer_state_dict' in checkpoint_data and config.get('continue_training', False):\n",
    "            #     optimizer.load_state_dict(checkpoint_data['optimizer_state_dict'])\n",
    "            logging.info(f\"Modelo cargado. Dtype parámetros: {next(model.parameters()).dtype}\") #\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al cargar el modelo pre-entrenado: {e}. Entrenando desde cero.\")\n",
    "            if not train_loader:\n",
    "                logging.error(\"No hay datos de entrenamiento y el modelo pre-entrenado no se pudo cargar. Saliendo.\")\n",
    "                return\n",
    "            model, history = train_model(model, train_loader, val_loader, config)\n",
    "    else:\n",
    "        if os.path.exists(model_path):\n",
    "             logging.info(f\"Modelo pre-entrenado en {model_path} existe, pero load_existing_model_if_available es False. Entrenando desde cero.\")\n",
    "        else:\n",
    "             logging.info(\"No se encontró modelo pre-entrenado. Entrenando desde cero...\") #\n",
    "        \n",
    "        if not train_loader: #\n",
    "            logging.error(\"No hay datos de entrenamiento y no se encontró/cargó modelo pre-entrenado. Saliendo.\") #\n",
    "            return\n",
    "        model, history = train_model(model, train_loader, val_loader, config) #\n",
    "\n",
    "    # 'model' ya es 'trained_model' en este punto.\n",
    "    model.to(device_for_execution) #\n",
    "    model.float() #\n",
    "    logging.info(f\"Modelo listo para predicción. Dtype: {next(model.parameters()).dtype}, Dispositivo: {next(model.parameters()).device}\") #\n",
    "\n",
    "    prediction_loader = val_loader if val_loader and val_dataset_len > 0 else train_loader #\n",
    "    # Ajustar num_prediction_samples para usar el tamaño del dataset si es menor que 5\n",
    "    if prediction_loader:\n",
    "        dataset_size_for_pred = len(prediction_loader.dataset)\n",
    "        num_prediction_samples = min(config.get('num_prediction_samples', 2), dataset_size_for_pred) # Pide 2 por defecto\n",
    "    else:\n",
    "        num_prediction_samples = 0\n",
    "\n",
    "    if prediction_loader and num_prediction_samples > 0: #\n",
    "        logging.info(f\"Generando {num_prediction_samples} predicciones de ejemplo...\") #\n",
    "        generate_prediction_netcdf(model, prediction_loader, config,\n",
    "                                   device=device_for_execution,\n",
    "                                   num_samples=num_prediction_samples) #\n",
    "    else:\n",
    "        logging.warning(\"No hay datos disponibles en val_loader o train_loader para generar predicciones de ejemplo.\") #\n",
    "\n",
    "    logging.info(\"Proceso completado.\") #\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Asegúrate de que todas las clases y funciones necesarias estén definidas arriba ---\n",
    "    # Ejemplo de cómo podrían estar estructuradas las importaciones si estuvieran en otro archivo\n",
    "    # from data_utils import RadarDataset \n",
    "    # from model_arch import ConvLSTMCell, ConvLSTM2DLayer, ConvLSTM3D_Enhanced \n",
    "    # from training_utils import train_model, SSIMLoss\n",
    "    # from prediction_utils import generate_prediction_netcdf\n",
    "    # from reproducibility_utils import set_seed\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec8c64-b9ab-4b1a-a742-d0bca5569527",
   "metadata": {
    "id": "7eec8c64-b9ab-4b1a-a742-d0bca5569527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyproj\n",
      "  Downloading pyproj-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from pyproj) (2024.8.30)\n",
      "Downloading pyproj-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyproj\n",
      "Successfully installed pyproj-3.7.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae001fd-5f38-499d-acdc-b5851649dbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
